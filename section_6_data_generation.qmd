# Generation and Analysis of Kinetics Data {#sec-6_kin_data_gen}

If the rate of a reaction has been studied previously, an expression for the rate of that reaction may be available. When a rate expression is [not]{.underline} available, it is necessary to develop one. In this situation, the mathematical form of the rate expression is not known. Whatever rate expression is eventually developed will contain kinetics parameters that initially are unknown. Kinetics data are needed to develop a rate expression because theory alone cannot provide the mathematical form of the rate expression nor the values of the kinetics parameters that appear in it.

Reaction rates depend upon temperature, pressure and composition. Temperature can be measured using thermometers or thermocouples. Pressure can be measured using manometers or a wide variety of pressure gauges. Fluid composition can be measured using gas chromatography, mass spectrometry or many other instruments. The same is not true for reaction rates. There isn't a meter, gauge, or instrument that one can put in or attach to a chemical reactor to directly measure the rate of the reaction or reactions going on within it. 

Since rates cannot be measured directly, changes in composition (or something related to composition) that occur in chemical reactors are measured instead. Herein, that measure quantity is referred to as the experimental response. The response is measured experimentally at different reactor temperatures, pressures and compositions, resulting in a set of experimental kinetics data. ***Reaction Engineering Basics* only considers the generation and analysis of kinetics data using isothermal experiments wherein one reaction is taking place and one response is measured.** Such data are sometimes called isothermal, single-response kinetics data.

This chapter first considers the design of experiments to generate isothermal, single response kinetics data. It then presents an overview of the use of such data to generate a rate expression. Before doing so, it is useful to recall the process used to develop rate expressions that was described in @sec-2_rates_rate_express.

## Procedure for Developing a Rate Expression

The following sequence of events is representative of the process for development of a rate expression for a reaction.

1. A preliminary analysis, perhaps including a few preliminary experiments, is performed to establish the range of conditions (temperature, pressure and composition) over which the rate expression will be used.
2. A laboratory reactor is selected to be used to generate kinetics data.
3. A set of kinetics experiments using that reactor is specified, and the experiments are performed to generate a kinetics data set.
4. A mathematical form is proposed for the rate expression and used to generate a model that predicts the response for an experiment.
5. The values of all unknown parameters appearing in the proposed rate expression are estimated using the experimental data set.
6. The accuracy of the proposed rate expression is assessed.
7. One of the following decisions is made.
    a. Accept the rate expression.
    b. Perform additional experiments and reassess the proposed rate expression using steps 5 through 7.
    c. Reject the proposed rate expression, propose another rate expression with a different mathematical form, and assess the new rate expression using steps 5 through 7.

The identification of the reaction and the preliminary analysis (step 1) can be driven by a variety of factors including a perceived business opportunity, regulatory mandates, etc. The selection of a reactor to use in kinetics experiments (step 2) is briefly considered in Chapters [-@sec-6_bstr_data_analysis], [-@sec-6_cstr_data_analysis], and [-@sec-6_pfr_data_analysis]. Chapters [-@sec-2_rates_rate_express] and [-@sec-2_mechanisms] described how a mathematical form of the rate expression can be postulated empirically, theoretically or mechanistically. The remainder of this chapter presents a general overview of the design of kinetics experiments (step 3), parameter estimation (step 5), accuracy assessment (step 6), and deciding whether to accept the rate expression (step 7).

## Design of Kinetics Experiments

The preliminary analysis establishes a range of temperature, pressure and composition that is suitable and appropriate for running the reaction. The purpose of kinetics experiments is to generate experimental data that span that range of temperature, pressure and composition with sufficient resolution to capture how temperature, pressure, and composition affect the rate.

The nature of kinetics experiments is relatively straightforward. In each experiment a group of reactor inputs are adjusted to pre-determined values. Those adjusted experimental input variables are referred to as *factors* when discussing the design of experiments. The person performing the experiment then measures the experimental response. (The experimenter could measure several different responses but *Reaction Engineering Basics only considers the analysis of data where a single response was measured.) Thus the data from one experiment consist of the values of the factors (adjusted input variables) and the corresponding experimental response. Performing many experiments then results in a kinetics data set. A kinetics data set can be thought of as a table where there is a column for each factor and a column for the experimental response, and each row contains the values of those quantities for one of the experiments that was performed.

Before experiments begin it is necessary to select the type of reactor to be used and **identify the response that will be measured in the experiments**. The response should be some easily measurable quantity that is related to the change in the composition of the reacting fluid from the start of the experiment to the point when/where the response was measured. There are many possibilities including the outlet or final concentration of a reagent, the conversion of a reactant, an outlet or final mole fraction of a reagent, etc.

The response can also be a property that is related to the concentration of a reagent or the overall composition. For example, if one reagent absorbs radiation of a specific frequency, the transmission of radiation of that frequency through a fixed distance within the reacting fluid can be related to the reagent's concentration. Similarly, the refactive index of a liquid mixture may be related to its composition.

The next step in generating a kinetics data set is to **decide which reactor inputs to use as factors**. The factors should be chosen so that the resulting kinetics data set will span the desired range of temperature, pressure and composition. Kinetics data are typically generated using small, laboratory-scale reactors. Through the use of a temperature controller, it is usually possible to design the experimental reactor so that it will operate isothermally at a temperature chosen by the reactor operator. A temperature controller is a device that continually monitors the temperature of the reacting fluid and adjusts the amount of heating or cooling provided to the reactor so that the temperature remains constant at the chosen value. This makes temperature an obvious choice for one of the factors.

The pressure is also an obvious choice for one of the factors. For liquid-phase reactions the pressure often does not affect the rate and does not need to be adjusted. For gas phase reactions it usually is straightforward to adjust the initial system pressure in a BSTR from experiment to experiment. With CSTRs and PFRs, it is most often possible to set the inlet pressure and operate the reactor with negligible pressure drop.

The choice of factors related to the composition is less obvious. That is, there are several experimental inputs that can be adjusted to set the composition of the system. For isothermal gas-phase reactions, setting the initial or inlet pressure sets the total molar concentration, as can be seen from the ideal gas law, @eq-pressure_concentration_ideal_gas. Thus, one option is to use the initial or inlet mole fractions of the reagents present in the system as factors. An alternative would be to use the initial molar amounts or the inlet molar flow rates of the reagents as factors.

$$
C_{\text{total}} = \frac{n_{\text{total}}}{V} = \frac{P}{RT}
$${#eq-pressure_concentration_ideal_gas}

However, it is important to recognize that the composition will change as the reaction proceeds. This means that an additional way to vary the composition is by using the batch reaction time or the flow reactor space time as a factor. The most important consideration in choosing the factors that affect composition is to ensure that by varying the factors it is possible to span the entire range of compositions of interest identified in the preliminary analysis. Whatever composition-related factors are used, it is important to vary the amounts of reactants *and products* when performing kinetics experiments.

Having selected the factors to be used during kinetics data generation, it becomes necessary to **decide how to vary the factors from one experiment ot the next**. The data set could be generated by randomly changing the factors from one experiment to the next, but this may not be a good choice. Typically reaction rates are much more sensitive to temperature than to pressure and composition. The Arrhenius expression indicates an exponential dependence of rate coefficients upon inverse temperature. This is much stronger than the composition dependence found in most rate expressions. Consequently, if composition and temperature are varied simultaneously, the strong temperature effect may mask weaker composition effects. Randomly changing the factors may fail to yield a sufficient number of experiments that separate temperature and composition effects.

A better approach is to select "levels" for each factor and to use an experimental design to change the levels from one experiment to the next. This results in an experimental data set that can be broken into several same-temperature "blocks," where each block consists of all of the experiments conducted at one of the temperature levels. Within each of the blocks, the temperature was the same, and only the pressure and composition are varying. As such, the effects of pressure and composition are more easily ascertained because the stronger effect of temperature is absent.

To design the experiments **a set of "levels" must be chosen** for each of the factors. These are the values to which the factors will be adjusted during the experiments. For example, it might be decided to use four temperature levels of 100, 110, 120 and 130 °C. That means that a set of experiments will be conducted at 100 °C, another set at 110 °C, and so on. Things that must be considered when choosing the levels for each factor include the time that is available for doing the experiments and the cost of the experiments (purchase of reagents, salaries for technicians performing the experiments, etc.). 

Sometimes there can be a trade off between the resolution of the experimental data and the time/cost of the experiments. A design with more levels should capture the effect of changing a factor more fully, but it will require more experimental time and cost more. A design with fewer levels will reduce the experimental time required and cost less, but it may not fully capture the effects of changing temperature, pressure, and composition.

The kinetics experiments entail adjusting each of the inputs to one of its levels, measuring the corresponding response, adjusting one or more of the inputs to a different one of it levels, measuring the response, etc. The final aspect of experimental design is **deciding which combinations of levels to use to generate the kinetics data**. There are a few ways to do this. A common one, sometimes called a *full factorial design*, simply includes every possible combination of the input levels. [Example -@sec-example_18_6_1] illustrates full factorial design of a kinetics experiment.

## Parameter Estimation

After proposing a rate expression and generating a mathematical model that predicts the response for a given set of factor levels, the experimental data are used to estimate the values of the rate expression parameters. Before this can be done, the parameters in the proposed rate expression must be identified. In most cases this is obvious, but the presence of an equilibrium constant in a rate expression is an exception. If an equilibrium constant appears in a rate expression, and the free energy data needed to calculate its value are known and available, that equilibrium constant is [not]{.underline} treated as a rate expression parameter because its value can be calculated.

In contrast, a mechanistic rate expression might contain an equilibrium constant for which free energy data are not available. Under these circumstances, the equilibrium constant is treated as rate expression parameter. Assuming a constant, but unknown, heat of reaction, an Arrhenius-like expression, @eq-equil_like_arrhenius, can be written for it, in which case the pre-exponential factor and the unknown heat of reaction are treated as rate expression parameters.

Having identified the rate expression parameters, and assuming that the data were [not]{.underline} generated by random variation of the factors, but that instead a fixed number of temperature levels was used, two approaches to estimating the rate expression parameters are available. In one approach, all of the rate expression parameters are estimated at the same time using all of the experimental data. In the other approach, some rate expression parameters are estimated using same-temperature blocks of data and then the remaining rate expression parameters are estimated using the first set of estimates. Each approach has advantages and disadvantages that will be discussed here.

When all of the rate expression parameters are estimated at the same time using the full data set, rate coefficients must be written in terms of the Arrhenius pre-exponential factor and activation energy, @eq-arrhenius. Similarly, equilibrium constants with unavailable free energy data must be written in terms of a pre-exponential factor and an unknown heat of reaction. The pre-exponential factors, activation energies, unknown heats of reaction, and any other rate expression parameters are then estimated by fitting the resulting model to the experimental data (see below). This is done using a numerical fitting function.

This approach is preferred because it uses [all]{.underline} of the data in estimating each of the rate expression parameters. A second advantage is that it works with rate expression parameters for which the temperature dependence is not known (e. g. exponents in power-law rate expressions). The main disadvantage is that typically a numerical fitting function must be used, and that requires that an initial guess for each rate expression parameter must be provided. The possible value of some rate expression parameters might span a range of 20 or more orders of magnitude, and if the initial guess is not close enough to the actual value, the fitting function may not be able to estimate the parameter values. Making an intial guess that is close enough to the actual value can be very challenging and can take a great deal of time.

In the second approach, the rate coefficients (not the pre-exponential factors and activation energies) and the equilibrium constants with unavailable free energy data (not the pre-exponential factors and unknown heats of reaction) are used in the model being fit to the each same-temperature block of data. For each parameter in the rate expression, this results in a set of its values at each of the temperature levels. That is, if the rate expression contains a rate coefficient, it results in a set of values of the rate coefficient at each of the experimental temperature levels.

The parameters in a model for the temperature dependence of each rate expression parameter then must estimated using the results. That is, for example, the Arrhenius pre-exponential factor and activation energy must be estimated frome the $k$ *vs.* $T$ results from analyzing the same-temperature data blocks. This is described in @sec-apndx_parameter_est, and illustrated in Examples [-@sec-example_4_5_4] and [-@sec-example_L_7_1].

One advantage of this approach is that [in some cases]{.underline} linear least squares can be used to estimate the parameters. This eliminates the need to provide an initial guess for the value of each parameter being estimated. Indeed, linear least squares parameter estimation can be performed using a spreadsheet program. A second advantage to this approach is that when the model being used to estimate the parameters is a differential equation, the model can be approximated as an algebraic equation, eliminating the need to solve the differential equation.

However, there are several disadvantages to this approach. The first is that only a fraction of the experimental data are used to estimate each rate expression parameter. A second significant disadvantage arises when the temperature dependence of a rate expression parameter is not known. As an example, consider a power-law rate expression, @eq-power_law_expr_conc. Parameter estimation using the same-temperature data blocks for each temperature level will yield $k$ *vs.* $T$ and each exponent, $\alpha_i$ *vs.* $T$.

The Arrhenius expression can be fit to the $k$ *vs.* $T$ data, but the temperature dependence of a power-law rate expression exponent is not known. If the estimated value of $\alpha_i$ was effectively the same at every temperature, it could be taken to be constant. However, if the estimated values of $\alpha_i$ vary with temperature it becomes necessary to find an expression that predicts its temperature dependence and assess the accuracy of that expression. There is no theoretical guidance in seeking an expression for the temperature dependence of a power-law rate expression exponent. If it wasn't possible to find an expression for the temperature dependence of $\alpha_i$, then a table of values as a function of temperature would be necessary. Interpolation would need to be used for temperatures not in the table. In this case, the rate expression would not be very useful, even if it was very accurate in representing the individual data blocks.

This suggests a third hybrid approach to parameter estimation. To start, the analysis of the same temperature data blocks would be used as described above. This would provide guesses for all of the rate expression parameters that could be used for parameter estimation using the full data set. During parameter estimation using the full data set, the pre-exponential factors, activation energies, unknown heats of reaction, and power-law exponents would be estimated. This woule result in parameters that were estimated from the full data set, and it would produce a single, constant best value for each rate expression parameter for which the temperature dependence is unknown.

### The Predicted Response Model

A model for the reactor used in the experiments can be generated as described in Chapters [-@sec-3_design_eqns] and [-@sec-3_reactor_model_func]. Assuming that each kinetics experiment was isothermal (and for PFRs assuming negligible pressure drop), only mole balances need to be included in the reactor design equations. The proposed rate expression will appear in the reactor design equations. Given values for the rate expression parameters and the values of the experimentally adjusted inputs for any one experiment, The reactor design equations can be solved to predict the final molar amounts (for a BSTR) or outlet molar flow rates (for a CSTR or PFR) for that experiment.

Solving the reactor design equations yields the predicted final/outlet molar amounts, i. e. the final composition, for an experiment. The experimentally measured response is related to the final reactor composition, so the results from solving the reactor model can be used to cacluate the model-predicted response for that experiment. This constitutes a predicted response model for an experiment. A predicted responses function simply loops through all of the experiments, calculating the model-predicted response for each experiment. Then it returns the resulting set of model-predicted responses. Of course, the predicted response model and the model-predicted responses function require as input, the rate expression parameters and the adjusted experimental input data.

### Estimation of Rate Expression Parameters

It was noted above that parameter estimation involves a process known as "fitting" a model to data. To estimate rate expression parameters the predicted responses model is "fit" to the kinetics data. Just as special solver functions are used to solve ATEs and IVODEs numerically, special numerical fitting functions are used to fit models to data. There are many software packages that provide fitting functions. Recognizing that different readers of this book will prefer to use different software packages, @sec-apndx_parameter_est describes how such software works, the input it requires and the results it returns in general terms. Sufficient information is provided so that readers of this book can follow the examples and can implement solutions using the software they prefer. It is left to the reader to consult the documentation for the software they choose to learn the details for using it.

@fig-param_est_info_flow illustrates the essence of using a numerical fitting function to fit a predicted responses model to experimental data. Briefly, the fitting function is provided with the experimental factors (adjusted input variables) and measured responses for each of the experiments being used in the analysis, an initial guess for the parameters, and the name of a predicted responses function that the user must write. In essence, the fitting function keeps guessing values for the parameters until it finds the parameter values for which the model-predicted responses are as close as possible to the experimentally measured responses (see @sec-apndx_parameter_est for more details). It then returns those parameter values along with statistical information that indicates how closely the experimental and predicted responses agree. Most *Reaction Engineering Basics* examples assume that the fitting function returns the 95% confidence intervals for each parameter and the coefficient of determination, $R^2$, as the statistical indicators (again, see @sec-apndx_parameter_est for more details). Other statistical indicators such as standard errors can be used in place of these.

![Information flow for fitting a predicted responses model to an experimental data set using a computer fitting function.](Graphics/parameter_estimation_info_flow.png){#fig-param_est_info_flow width="70%"}

Internally, the fitting function needs to be able to calculate the responses predicted by the model each time it generates a new guess for the parameters. To enable it to do so, the user must write a computer function that will be referred to here as the **predicted responses function**, and provide it to the numerical fitting function. As indicated in @fig-param_est_info_flow, that function will be provided with guesses for the parameters and the factors for all of the experiments in the data set being analyzed. It must calculate and return the corresponding model-predicted responses for the experimental data set.

### Reactor Models Used in Responses Functions

Personal computers now are ubiquitous, and a variety of mathematics software packages with fitting functions are available. As such, it is relatively easy and straightforward to solve the reactor model equations numerically within the predicted responses function. Doing so is virtually always possible, and the reactor models are the same as those used everywhere else in this book. This approach is used in most of the examples in *Reaction Engineering Basics* because it uses the full data set to estimate each of the parameters, it can always be used, and it does not introduce any inaccuracies. The downsides are that it requires writing computer code and making an initial guess for the parameter values.

As previously noted, an alternative approach involves analyzing same-temprerature data blocks spearately and then fitting a model for the temperature dependence of the rate parameters to the results. This approach does not use the full data set to estimate each parameter. It's primary advantage is that in some instances, it facilitates the "liearization" of the predicted response model, and that, in turn eliminates the need to use a numerical fitting function. When the model can be linearized, fitting can be performed using a calculator or a spreadsheet program. Its major drawbacks are that it cannot always be used, it does not use the full data set to estimate each parameter, and it is not well-suited to power-law and other rate expressions where the temperature dependence of the rate expression parameters is unknown. 

In one variant of this second approach that only applies when the mole balances are differential equations offers the additional advantage of being able to approximate the derivatives and avoid solving the differential equations. Doing so does introduce some error associated with the approximation of the derivatives.

Historically, i. e. when the author was a student, computers were not readily available and both solving the reactor model equations and fitting the model to experimental data had to be performed analytically using a calculator or even a slide rule. Under these conditions, the analysis of same-temperature data blocks, made the calculations tractable.

The second approach where same-temperature data blocks are analyzed separately is illustrated in a couple of examples in *Reaction Engineering Basics,* because it is still widely taught and used. Nonetheless, in light of the fact that it can't always be used and its other disadvantages, parameter estimation using the full data set and a numerical fitting function is emphasized in this book.

#### Modifying the Reactor Model to Facilitate Guessing Pre-Exponential Factors

One disadvantage of using a numerical fitting function is the need to provide an initial guess for the rate expression paramters. This is not too difficult for activation energies, unknown heats of reactor, and power-law exponents because their possible values span a relatively small range. However, the range of possible values for pre-exponential factors spans ten or more orders of magnitude.

If the initial guess for a pre-exponential factor is many orders of magnitude different from its actual value, a numerical fitting function will likely fail, because it generates new guesses by perturbing the value of the provided guess. When the perturbed guess is essentially just as far off as the initial guess, the numerical fitting function can't tell which guess is better due to numerical round off and other factors.

One way to remedy this situation is to use the numerical fitting function to estimate the base-10 logarithm of the pre-exponential factor. By doing so, the possible values span a range of 10 to 20 instead of 10 to 20 orders of magnitude. When the numerical fitting function perturbs the guess, it is more likely to be able to determine whether the perturbed guess is better or worse than the initial guess.

If a numerical fitting function is used to estimate the base-10 log of a pre-exponential factor, two things are critically important. First, within the model, the base-10 log of the pre-exponential factor must be converted to the actual pre-exponential factor. Second, when the numericaly fitting function returns the best estiate for the base-10 log of the pre-exponential factor and the 95% confidence interval for the base-10 log of the pre-exponential factor, they must be converted to an estimate for the actual pre-exponential factor and its 95% confidence interval.

## Accuracy Assessment {#sec-accuracy_assessment}

Accuracy must be assessed each time parameter estimation is performed. All of the approaches described above for performing parameter estimation can also be made to yield additional statistics that are useful for assessing the accuracy of the resulting rate expression. These include some measure of the uncertainty in each of the estimated parameters (typically the standard error or a 95% confidence interval) and the coefficient of determination, $R^2$. The following criteria indicate an accurate rate expression.

* The coefficient of determination, $R^2$, is close to 1.0.
* The uncertainty in each parameter is small relative to its value.
    * The standard error for the parameter is small relative to its value.
    * The upper and lower extremes of the 95% confidence interval for the parameter are close to the estimated value of the parameter.

Ideally, the uncertainty for every parameter should be small. However it sometimes results that while the uncertainty in *most* of the parameters is amall, a few parameters may have large uncertainties. For example, quite often the uncertainty in an Arrhenius pre-exponential factor is quite large. A large uncertainty for a parameter could indicate one of three possibilities. First, the factor levels used in the experiments may not allow accurate resolution of the parameters with high uncertainty. Second, the parameters with higher uncertainty may be mathematically coupled to other parameters (e. g. the rate may only depend on the product of two parameters so that the individual parameters can have any values as long as their product has the optimum value). Alternatively, the parameters with high uncertainty may not be needed, and there may be a simpler rate expression that is equally accurate with fewer parameters.

Graphical assessment of model accuracy is also possible and can be particularly helpful in deciding whether a high uncertainty in one or two rate expression parameters is a cause for concern. Two types of graphs referred to here as parity plots and residuals plots are useful. To generate these graphs, the predicted parameter values are first used to calculate the model-predicted responses for all of the experiments in the data set.

A **parity plot** is constructed by plotting the experimental responses *vs*. the model-predicted responses as points. A diagonal line, corresponding to the the experimental responses being equal to the model-predicted responses is than added to the graph. The closer the points are to the line, the higher the accuracy of the rate expression.

To generate **a set of residuals plots**, a residual is calculated for each experimental data point. The residual is the difference between the experimental response for a data point and the corresponding model-predicted response. Plotting the set of residual *vs.* each of the adjusted experimental inputs yields the set of residuals plots.

In general, the parity plot is used to gauge the accuracy of the model while the residuals plots are examined to see whether there are systematic trends in the residuals. The points in a residuals plot should scatter randomly about zero. If a systematic trend is observed, it may suggest that the adjusted input used to generate the plot has an effect upon the rate that is not being accuratly captured by the rate expression. Thus, in graphical assessment, the following criteria suggest that the model is accurate.

* The points in the parity plot are all close to a diagonal line ($y_{\text{expt}} = y_{\text{model}}$).
* In each residuals plot, the points scatter randomly about zero (the horizontal axis), and no systematic deviations are apparent.

When same-temperature data blocks are analyzed separately using a linearized model, graphical assessment makes use of a **model plot** and an **Arrhenius plot** for each rate coefficient or equilibrium constant with unavailable free energy data. When a model plot is used, both the magnitude of the scatter of the data from the model line and the occurrence of systematic trends are the focus.

## Deciding Whether to Accept the Proposed Rate Expression

The final step in kinetics data analysis involves **making a decision** to accept, reassess with additional data, or reject the proposed rate expression. In the end, this is a judgement call, and it becomes easier as a reaction engineer gains experience.  The intended use of the rate expression, and more importantly the potential consequences of accepting an inaccurate rate expression, should be given serious consideration when making this decision. That is, if accepting an inaccurate rate expression might result in severe personal injury, significant property damage or catastrophic financial loss, the rate expression should be very, very accurate if it is accepted. If the consequences of accepting an inaccurate rate expression are less severe, somewhat lower accuracy may be deemed acceptable.

## Examples

This chapter described the generation and analysis of kinetics data. The analysis of kinetics data was described in general terms. That information will be applied to the analysis of data from BSTRs, CSTRs and PFRs and examples will be presented in the next three chapters. The examples presented here can be applied to data generation and analysis using any of the ideal reactor types. [Example -@sec-example_18_6_1] illustrates the design of kinetics experiments for the purpose of generating kinetics data. [Example -@sec-example_18_6_2] shows how to modify the model used by a numerical fitting function to estimate the base-10 logarithm of a pre-exponential factor, and how to process the results returned by the numerical fitting function.

### Design of Kinetics Experiments {#sec-example_18_6_1}

{{< include examples/reb_18_6_1/narrative.qmd >}}

---

:::{.callout-tip collapse="true"}
## Click Here to See What an Expert Might be Thinking at this Point

I am asked to design kinetics experiments, so I need to decide which variables will be adjusted in the experiments. I further need to decide how many levels to use for each adjusted variable and what those levels should be. The problem states that the rate expression developed using these data will be used to design a new process, so I want to be sure to generate a sizeable data set that spans the expected ranges of the adjusted variables and also captures the effects of each of them upon the response. Based upon the information presented in the problem statement, the experimental response here will be the concentration of A.

Reaction rates can be affected by the temperature and the concentration of each reagent present. Here the problem states that the rate is not affected by the concentration of Z, so the temperature and the concentration of A should vary from one experiment to the next. The reactor is isothermal, so the temperature can be adjusted directly in the experiments. The concentration of A will change as the reaction proceeds. This suggests two ways to vary the concentration of A from experiment to experiment. The first is to adjust the initial concentration of A and the second is to adjust the time at which the response is measured. Longer times will lead to smaller concentrations of A because at longer times more of the A will have reacted. I will use both the initial concentration of A and the reaction time as adjusted variables.

Next I need to decide how many levels to use for each adjusted variable and what those levels should be. Normally I would use levels that span a slightly wider range than the range where the rate expression will be used. Here, however, I'm told that the rate is too low below 65 °C and undesirable reactions occur above 90 °C, so I will choose levels that just span that range. The range only spans 25 °C, so four temperature levels seem reasonable, as does spacing them equally across the range.

I want to span a range of concentrations that is slightly wider than the expected range where the rate expression will be used. Three initial concentration levels of 0.5, 1.0, and 1.5 M will do so. Then, in order to ensure that the data are sensitive to the effect of the concentration of A, I will use six levels of reaction time. Noting that at 80 °C it takes 30 min for the reaction to go to completion, spacing the reaction times 5 minutes apart will lead to samples that span a wide range of conversions, and hence a wide range of concentrations of reagent A.

:::

**Experimental Design**

Three reactor inputs will be adjusted in the experiments: the temperature, $T$, the initial concentration of A, $C_{A,0}$, and the reaction time, $t$. The temperature levels will be 65, 73, 82, and 90 °C; the initial concentration levels will be 0.5, 1.0, and 1.5 M; the reaction time levels will be 5, 10, 15, 20, 25, and 30 min. All possible combinations of these levels will be studied giving a total of 72 experimental data points.

It will not be necessary to perform 72 experiments, however. Using a reactor at 65 °C with an initial concentration of A equal to 0.5 M, six responses can be recorded in the experiment (at reaction times of 5, 10, 15, 20, 25, and 30 min). The number of experiments needed to record all 72 responses is 12. The initial conditions for those 12 experiments are shown in @tbl-example_18_6_1. Each experiment in the table will yield responses at all six reaction time levels.

| Experiment | T (°C) | C~A,0~ (M) |
|:------:|:-------:|:----------:|
| 1 | 65 | 0.5 |
| 2 | 65 | 1.0 |
| 3 | 65 | 1.5 |
| 4 | 73 | 0.5 |
| 5 | 73 | 1.0 |
| 6 | 73 | 1.5 |
| 7 | 82 | 0.5 |
| 8 | 82 | 1.0 |
| 9 | 82 | 1.5 |
| 10 | 90 | 0.5 |
| 11 | 90 | 1.0 |
| 12 | 90 | 1.5 |

: Initial conditions for kinetics experiments. {#tbl-example_18_6_1 tbl-colwidths="[30,35,35]"}

:::{.callout-note collapse="false"}
## Note

Some readers might have difficulty with this problem because it relies upon an understanding of how BSTR kinetics experiments are performed and the data they generate, and that information has not been presented yet. BSTR kinetics experiments are considered in @sec-6_bstr_data_analysis. The important aspects of this example are that it was first necessary to decide which reactor model input variables to adjust, and then it was necessary to decide how many levels of each variable to use and what those levels should be. After doing that, the experimental design simply involves performing an experiment at every combination of the adjusted variable levels.

:::

### Estimating the Base-10 Logarithm of a Pre-Exponential Factor {#sec-example_18_6_2}

A reaction engineer has been tasked with fitting the model shown in equation (1) to a set of experimental data. In that model, $\dot{n}_{A,f}$ is the response, $C_{A,0}$ and $dot{V}$ are the adjusted experimental inputs, $V$ and $R$ are known constants, and $k_0$ and $E$ are the rate expression parameters that must be estimated. When the engineer attempted to use a numerical fitting function to complete the task, it failed to converge, yielding unacceptably poor parameter estimates. How can the model be modified to improve the likelihood of obtaining good parameter estimates, and how does the modification affect the use of the numerical fitting function?

$$
0 = \dot{V}C_{A,0} - \dot{n}_{A,f} - k_0 \exp{\left( \frac{-E}{RT} \right)}\frac{\dot{n}_{A,f}}{\dot{V}} \tag{1}
$$

---

The problem with using a numerical fitting function to estimate $k_0$ and $E$ in the given model is that a guess must be provided for $k_0$. The actual value of $k_0$ could be anywhere in the range from 10^-20^ to 10^20^, making extremely difficult to make an ititial guess.

The situation will be improved by defining $\beta$ as shown in equation (2), and re-writint the model as shown in equation (3).

$$
\beta = \log_{10} \left(k_0\right) \tag{2}
$$

$$
0 = \dot{V}C_{A,0} - \dot{n}_{A,f} - 10^\beta \exp{\left( \frac{-E}{RT} \right)}\frac{\dot{n}_{A,f}}{\dot{V}} \tag{3}
$$

The numerical fitting function now can be used to estimate $\beta$ instead of $k_0$. The range of possible values of $\beta$ is between -20 and +20. This makes it easier to make a guess, and it makes it more likely that when the numerical fitting function will be able to estimate $\beta$.

Assuming the numerical fitting function is successful, it will return the best estimate for $\beta$. The best estimate for $k_0$ can be found using equation (4).

$$
\beta = \log_{10} \left(k_0\right) \qquad \Rightarrow \qquad k_0 = 10^\beta \tag{4}
$$

The numerical fitting function will also return a measure of the uncertainty in $\beta$. If it returns the upper and lower limits of the 95% confidence interval for $\beta$, denoted as $\beta_{CI,l}$ and $\beta_{CI,u}$ the upper and lower limits of the confidence interval for $k_0$, $k_{0,CI,l}$ and $k_{0,CI,u}$, can be calculated using equations (5) and (6). If the numerical fitting function returns the stardard error for $\beta$, $\lambda_\beta$, the standard error for $k_0$, $\lambda_{k_0}$ can be calculated using equation (7).

$$
k_{0,CI,l} = 10^{\beta_{CI,l}} \tag{5}
$$

$$
k_{0,CI,u} = 10^{\beta_{CI,u}} \tag{6}
$$

$$
\lambda_{k_0} = k_0 \ln \left(10\right) \lambda_\beta \tag{7}
$$

## Symbols Used in @sec-6_kin_data_gen

| Symbol | Meaning |
|:-------|:--------|
| $k$ | Rate coefficient, an additional index is used to denote the reaction if more than one reaction is taking place. |
| $k_0$ | Pre-exponential factor in the Arrhenius expression. |
| $n_i$ | Molar amount of reagent $i$; an additional subscript denotes the time. |
| $\dot{n}_i$ | Molar flow rate of reagent $i$; an additional subscript denotes the location. |
| $C_i$ | Concentration of reagent $i$; an additional subscript denotes the time or location. |
| $E$ | Activation energy in the Arrhenius expression. |
| $P$ | Pressure. |
| $R$ | Ideal gas constant. |
| $R^2$ | Coefficient of determination. |
| $T$ | Temperature. |
| $V$ | Volume. |
| $\dot{V}$ | Volumetric flow rate. |

: {tbl-colwidths="[20,80]"}