# Generation and Analysis of Kinetics Data {#sec-6_kin_data_gen}

If the rate of a reaction has been studied previously, an expression for the rate of that reaction may be available. When a rate expression is [not]{.underline} available, it is necessary to develop one. In this situation, the mathematical form of the rate expression is not known. Whatever rate expression is eventually developed will contain kinetics parameters that initially are unknown. Kinetics data are needed to develop a rate expression because theory alone cannot (yet) provide the mathematical form of the rate expression nor the values of the kinetics parameters that appear in it.

Reaction rates depend upon temperature, pressure and composition. Temperature can be measured using thermometers or thermocouples. Pressure can be measured using manometers or a wide variety of pressure gauges. Fluid composition can be measured using gas chromatography, mass spectrometry or many other instruments. The same is not true for reaction rates. There isn't a meter, gauge, or instrument that one can put in or attach to a chemical reactor to directly measure the rate of the reaction or reactions going on within it. 

Since rates cannot be measured directly, changes in composition (or something related to composition) that occur in chemical reactors are measured instead. Herein, that measured quantity is referred to as the experimental response. The response is measured experimentally at different reactor temperatures, pressures and compositions, resulting in a set of experimental kinetics data. *Reaction Engineering Basics* only considers the generation and analysis of kinetics data using isothermal experiments wherein one reaction is taking place, only one response is measured in each experiment, and the response is the same quantity in every experiment. Such data are sometimes called isothermal, single-response kinetics data.

This chapter first considers the design of experiments to generate isothermal, single response kinetics data. It then presents an overview of the use of such data to generate a rate expression. Before doing so, it is useful to recall the process used to develop rate expressions that was described in @sec-2_rates_rate_express.

## Procedure for Developing a Rate Expression

The following sequence of events is representative of the process for development of a rate expression for a reaction.

1. A preliminary analysis, perhaps including a few preliminary experiments, is performed to establish the range of conditions (temperature, pressure and composition) over which the rate expression will be used.
2. A laboratory reactor is selected to be used to generate kinetics data.
3. A set of kinetics experiments using that reactor is specified, and the experiments are performed to generate a kinetics data set.
4. A mathematical form is proposed for the rate expression and used to generate a model that predicts the response for an experiment.
5. The values of all unknown parameters appearing in the proposed rate expression are estimated by "fitting" that model to the experimental data set.
6. The accuracy of the proposed rate expression is assessed.
7. One of the following decisions is made.
    a. Accept the rate expression.
    b. Perform additional experiments and reassess the proposed rate expression using steps 5 through 7.
    c. Reject the proposed rate expression, propose another rate expression with a different mathematical form, and assess the new rate expression using steps 5 through 7.

The identification of the reaction and the preliminary analysis (step 1) can be driven by a variety of factors including a perceived business opportunity, regulatory mandates, etc. The selection of a reactor to use in kinetics experiments (step 2) is briefly considered in Chapters [-@sec-6_bstr_data_analysis], [-@sec-6_cstr_data_analysis], and [-@sec-6_pfr_data_analysis]. Chapters [-@sec-2_rates_rate_express] and [-@sec-2_mechanisms] describe how a mathematical form of the rate expression can be postulated empirically, theoretically, or mechanistically. The remainder of this chapter presents a general overview of the design of kinetics experiments (step 3), generation of a model that predicts the response for an experiment (step 4), parameter estimation (step 5), accuracy assessment (step 6), and deciding whether to accept the rate expression (step 7).

## Design of Kinetics Experiments

The preliminary analysis establishes a range of temperature, pressure and composition that is suitable and appropriate for running the reaction. The purpose of kinetics experiments is to generate experimental data that span that range of temperature, pressure and composition with sufficient resolution to capture how temperature, pressure, and composition affect the rate.

The nature of kinetics experiments is relatively straightforward. In each experiment a group of reactor inputs are adjusted to pre-determined values. Those adjusted experimental input variables are referred to as *factors* when discussing the design of experiments. The person performing the experiment then measures the experimental response. Thus the data from one experiment consist of the values of the factors (adjusted experimental input variables) and the corresponding experimental response. Performing many experiments then results in a kinetics data set. A kinetics data set can be thought of as a table where there is a column for each factor and a column for the experimental response, and each row contains the values of those quantities for one of the experiments that was performed.

Before experiments begin it is necessary to select the type of reactor to be used and **identify the response that will be measured in the experiments**. The response should be some easily measurable quantity that is related to the change in the composition of the reacting fluid from the start of the experiment to the point when/where the response was measured. There are many possibilities including the outlet or final concentration of a reagent, the conversion of a reactant, an outlet or final mole fraction of a reagent, etc.

The response can also be a property that is related to the concentration of a reagent or the overall composition. For example, if one reagent absorbs radiation of a specific frequency, the transmission of radiation of that frequency through a fixed distance within the reacting fluid can be related to the reagent's concentration. Similarly, the refactive index of a liquid mixture may be related to its composition.

The next step in generating a kinetics data set is to **decide which reactor inputs to use as factors**. The factors should be chosen so that the resulting kinetics data set will span the desired range of temperature, pressure and composition. Kinetics data are typically generated using small, laboratory-scale reactors. Through the use of a temperature controller, it is usually possible to design the experimental reactor so that it will operate isothermally at a temperature chosen by the reactor operator. A temperature controller is a device that continually monitors the temperature of the reacting fluid and adjusts the amount of heating or cooling provided to the reactor so that the temperature remains constant at the chosen value. This makes temperature an obvious choice for one of the factors.

The pressure is also an obvious choice for one of the factors. For liquid-phase reactions the pressure often does not affect the rate and does not need to be adjusted. For gas phase reactions it usually is straightforward to adjust the initial system pressure in a BSTR from experiment to experiment. With CSTRs and PFRs, it is most often possible to set the inlet pressure and operate the reactor with negligible pressure drop.

The choice of factors related to the composition is less obvious. That is, there are several experimental inputs that can be adjusted to set the composition of the system. For isothermal gas-phase reactions, setting the initial or inlet pressure sets the total molar concentration, as can be seen from the ideal gas law, @eq-pressure_concentration_ideal_gas. Thus, one option is to use the initial or inlet pressure together with the initila or inlet mole fractions of the reagents present in the system as factors. An alternative would be to use the initial molar amounts or the inlet molar flow rates of the reagents as factors.

$$
C_{\text{total}} = \frac{n_{\text{total}}}{V} = \frac{P}{RT}
$${#eq-pressure_concentration_ideal_gas}

It is important to recognize that the composition will change as the reaction proceeds. This means that an additional way to vary the composition is by using the batch reaction time or the flow reactor space time as a factor. The most important consideration in choosing the factors that affect composition is to ensure that by varying the factors it is possible to span the entire range of compositions of interest identified in the preliminary analysis. Whatever composition-related factors are used, it is important to vary the amounts of reactants [and products]{.underline} when performing kinetics experiments.

Having selected the factors to be used during kinetics data generation, it becomes necessary to **decide how to vary the factors from one experiment ot the next**. The data set could be generated by randomly changing the factors from one experiment to the next, but this may not be a good choice. Typically reaction rates are much more sensitive to temperature than to pressure and composition. (The Arrhenius expression indicates an exponential dependence of rate coefficients upon inverse temperature, an this is much stronger than the composition dependence found in most rate expressions.) Consequently, if composition and temperature are varied simultaneously, the strong temperature effect may mask weaker composition effects. Randomly changing the factors may fail to yield a sufficient number of experiments that separate temperature and composition effects.

A better approach is to select "levels" for each factor and to use an experimental design to change the levels from one experiment to the next. This results in an experimental data set that can be broken into several same-temperature "blocks," where each block consists of all of the experiments conducted at one of the temperature levels. Within each of the blocks, the temperature is the same, and only the pressure and composition vary from experiment to experiment. As such, the effects of pressure and composition are more easily ascertained because the stronger effect of temperature is absent.

To design the experiments **a set of "levels" must be chosen** for each of the factors. These are the values to which the factors will be adjusted during the experiments. For example, it might be decided to use four temperature levels of 100, 110, 120 and 130 °C. That means that a set of experiments will be conducted at 100 °C, another set at 110 °C, and so on. Things that must be considered when choosing the levels for each factor include the time that is available for doing the experiments and the cost of the experiments (purchase of reagents, salaries for technicians performing the experiments, etc.). 

Sometimes there can be a trade off between the resolution of the experimental data and the time/cost of the experiments. A design with more levels should capture the effect of changing a factor more fully, but it will require more experimental time and cost more. A design with fewer levels will reduce the experimental time required and cost less, but it may not fully capture the effects of changing temperature, pressure, and composition.

The kinetics experiments entail adjusting each of the inputs to one of its levels, measuring the corresponding response, adjusting one or more of the inputs to a different one of it levels, measuring the response, etc. The final aspect of experimental design is **deciding which combinations of levels to use to generate the kinetics data**. There are a few ways to do this. A common one, sometimes called a *full factorial design*, simply includes every possible combination of the input levels. [Example -@sec-example_18_6_1] illustrates full factorial design of a kinetics experiment.

## Predicted Responses Model

The analysis of a set of kinetics data set requires a mathematical model that uses the rate expression being developed to predict the responses for the experiments that were performed. Since the mathematical form of the rate expression is unknown, one must be proposed (step 4 above). Ways of doing so are described in Chapters [-@sec-2_rates_rate_express] and [-@sec-2_mechanisms]. 

Once the mathematical form of the rate expression has been proposed, a model for the reactor used in the experiments can be created. This reactor model is no different from the reactor models generated and used in Chapters [-@sec-3_reactor_model_func], [-@sec-4_response_opt_design], [-@sec-4_bstr_analysis], [-@sec-4_cstr_analysis], and [-@sec-4_pfr_analysis]. It includes the reactor design equations, either initial guesses or initial values and a stopping criterion, either a derivatives function or a residuals function, and a solver for the design equations. In essence, each experiment can be thought of as a reactor response modeling task where the quantity of interest is the quantity that was measured as the experimental response.

The predicted responses model is given the adjusted inputs for all of the experiments and values for all of the parameters that appear in the proposed rate expression. In turn, it provides the adjusted inputs for each experiment to the reactor model, uses the reactor model to solve the design equations, and uses the results to calculate a predicted response. After doing this for all of the experiments, it returns the full set of model-predicted responses.

## Parameter Estimation

Parameter estimation can be thought of as a trial and error process wherein a set of value for the rate expression parametres is guessed and the model-predicted responses function is used to calculate a set of model predicted responses that is compared to the experimentally measured responses. The process is repeated until a set of parameter values is found that gives the best agreement between the model-predicted responses and the experimental responses. A few more details about parameter estimation are provided in @sec-apndx_parameter_est.

There are three kinds of parameters found in the rate expressions considered in *Reaction Engineering Basics*. Rate coefficients are expected to depend upon temperature according to the Arrhenius expression, @eq-arrhenius, and as such, each rate coefficient introduces two parameters to the rate expression, a pre-exponential factor and an activation energy.

Equilibrium constants sometimes appear in rate expressions. If the thermodynamic data (Gibbs free energy, etc.) needed to calculate a rate expression are available, the equilibrium constant is not treated as a rate expression parameter. However, in some situations, for example, when using a mechanistic rate expression, the thermodynamic data needed to calculate an equilibrium constant may not be known. Equilibrium constants for which thermodynamic data do not exist can be treated as rate expression parameters. They are expected to depend upon temperature according to an Arrhenius-like expression, @eq-equil_like_arrhenius, and so, when they appear in a rate expression, they also introduce two parameters: a pre-exponential factor and and unknown heat of reaction.

The third kind of parameter found in some rate expressions is an exponent that appears in a power-law rate expression, @eq-power_law_expr_conc or @eq-power_law_expr_part_press. Unlike rate coefficients and equilibrium constants, power-law exponents do not have a theoretical basis and are not expected to depend upon temperature in any particular manner. This becomes important when analyzing same-temperature data blocks as described below.

### Parameter Estimation Using the Full Data Set

In the preferred approach to parameter estimation, the predicted responses model is fit to the entire experimental data set. As described in @sec-apndx_parameter_est, a numerical fitting function is used to do this. Four things must be provided to a numerical fitting function: (1) the adjusted experimental inputs for all of the expriments, (2) the experimentally measured responses for all of the experiments, (3) an initial guess for every parameter in the rate expression, and (4) a predicted responses function. 

Parameter estimation using the full experimental data set is preferred because it uses [all]{.underline} of the data in estimating each of the rate expression parameters. Another advantage of using a numerical fitting function is that it can be used with virtually any proposed rate expression. Two disadvantages are that the success of a numerical fitting function can be very sensitive to the initial guesses for the rate expression parameters, and it requires writing a predicted responses function.

Sensitivity to the initial guesses can be particularly acute for pre-exponential factors. The possible range of values for an Arrhenius pre-exponential factor spans tens of orders of magnitude. An unknown Arrhenius pre-expoential factor could have a value anywhere in the range from 10^-20^ to 10^20^ or wider, and if the initial guess is not sufficiently close to the true value, the numerical fitting function will fail. One way to address this is to define a new parameter that is equal to the base-10 logarithm of the pre-exponential factor. Doing so reduces the range of possible values to between -20 and +20 and improves the likelihood that the numerical fitting function will succeed. In other words, instead of using a numerical fitting function to find the best value for an Arrhenius pre-exponential factor, it is used to find the best value for the base-10 log of that pre-exponential factor. Of course, the predicted responses function must be modified accordingly.

In a similar vein, success can be sensitive to guesses for Arrhenius activation energies because they appear in exponentials. Using small activation energies as initial guesses can improve the likelihood of a numerical fitting function being successful. Some of the examples in later chapters discuss issues associated with initial guesses for rate expression parameters.

When a numerical fitting function is successful, it will return the best estimate for each parameter value, a measure of the uncertainty in each estimated parameter value, the coefficient of determination, and, sometimes, the model predicted responses resulting from the estimated parameter values. The measure of uncertainty in the estimated parameter values typically takes the form of a standard error or a 95% confidence interval. If the numerically fitting function does not return the model-predicted responses for the estimated parameters, they can easily be obtained by calling the predicted responses function directly.

### Parameter Estimation Using Same-Temperature Data Sets

One very popular approach to parameter estimation proceeds in two phases. To use it, the data set is broken into same-tempeature blocks of data. That is, all of the data in a block have the same experimental temperature and no other blocks have data with that temperature. In the first phase of parameter estimation, each of the same-temperature data blocks is analyzed separately. 

It is not possibe to estimate the pre-exponential factor and activation energy for a rate coefficient or the pre-exponential factor and heat of reactor for an equilibrium constant using same-temperature data. Therefore, the analysis of each same-temperature data block yields estimates for each rate coefficient, equilibrium constant, and power-law exponent at the temperature of that data block. Analyzing all of the same-temperature data blocks thereby yields secondary data sets consisting of the values of each rate coefficient, equilibrium constant and power-law exponent at each data block temperature.

In the second phase of the analysis the Arrhenius expression is fit to the rate coefficient *vs.* temperature data from the first phase to estimate the pre-exponential factor and activation energy for each rate coefficient in the proposed rate expression. Doing this is ilustrated in [Example -@sec-example_4_5_4], and a dedicated function for performing the analysis is described in @sec-apndx_parameter_est. For each unknown equilibrium constatnt appearing in the proposed rate expression, an Arrhenius-like expression is fit to the equilibrium coefficient *vs.* temperature data from the first phase to estimate the pre-exponential factor and heat of reaction.

As mentioned above, the temperature dependence of a power-law rate expression exponent is not known. If the estimated value of a power-law exponent is effectively the same at every temperature, it can be taken to be constant. However, if the estimated value of a power-law exponent varies with temperature it becomes necessary to find an expression that predicts its temperature dependence. There is no theoretical guidance in seeking an expression for the temperature dependence of a power-law rate expression exponent. If an expression for the temperature dependence of a power-law exponent cannot be found, then a table of values as a function of temperature would be necessary. Interpolation would need to be used for temperatures not in the table. In this case, the rate expression would not be very useful, even if it was very accurate in representing the individual data blocks.

Analyzing same-temperature data blocks is most advantageous if the predicted responses model can be linearized. When it can be linearized, linear least squares can be used to estimate the parameters. This eliminates the need to provide an initial guess for the value of each parameter being estimated and the need to write a model-predicted responses function. In fact, linear least squares parameter estimation can be performed using a spreadsheet program.

Unfortunately, for many rate expressions it is not possible to linearize the predicted responses model, and in that situation, analysis using same-temperature data blocks doesn't offer much of an advantage over analysis of the ful data set. Even when the predicted responses model can be linearized, there is a second disadvantage. Specifically, when same-temperature data block analysis is used, only a fraction of the experimental data are used to estimate each rate expression parameter. The pre-exponential factors and activations are not estimated using any data; they are estimated using estimates from the first phase of the analysis.

Before computers were widely available, parameter estimation using same-temperature was used extensively, and it is still widely taught and used today. A similar situation exists for situations where the reactor model is a differential equation (i. e. when a BSTR or PFR is used in the experiments). Linear least squares requires that the reactor design equation be integrated analytically, and this often is not possible. Historically one approach to dealing with this was to approximate the derivative, eliminating the need for analytical integration of the rate expression. This is known as differential data analysis, and it is illustrated in @sec-6_bstr_data_analysis.

## Accuracy Assessment {#sec-accuracy_assessment}

Accuracy must be assessed each time parameter estimation is performed. All of the approaches described above for performing parameter estimation can also be made to yield additional statistics that are useful for assessing the accuracy of the resulting rate expression. These include some measure of the uncertainty in each of the estimated parameters (typically the standard error or a 95% confidence interval) and the coefficient of determination, $R^2$. The following criteria indicate an accurate rate expression.

* The coefficient of determination, $R^2$, is close to 1.0.
* The uncertainty in each parameter is small relative to its value.
    * The standard error for the parameter is small relative to its value.
    * The upper and lower extremes of the 95% confidence interval for the parameter are close to the estimated value of the parameter.

Ideally, the uncertainty for [every]{.underline} parameter should be small. However it sometimes results that while the uncertainty in *most* of the parameters is small, a few parameters may have large uncertainties. For example, quite often the uncertainty in an Arrhenius pre-exponential factor is quite large. If all other criteria indicate that the model is accurate, the rate expression may be deemed to be sufficiently accurate even if the uncertainty in a pre-exponential factor is significant.

A large uncertainty for a parameter could indicate one of three possibilities. First, the factor levels used in the experiments may not allow accurate resolution of the parameters with high uncertainty. Second, the parameters with higher uncertainty may be mathematically coupled to other parameters (e. g. the rate may only depend on the product of two parameters so that the individual parameters can have any values as long as their product has the optimum value). Alternatively, the parameters with high uncertainty may not be needed, and there may be a simpler rate expression that is equally accurate with fewer parameters.

Graphical assessment of model accuracy is also possible and can be particularly helpful in deciding whether a high uncertainty in one or two rate expression parameters is a cause for concern. Two types of graphs, referred to here as parity plots and residuals plots, are useful. To generate these graphs, the predicted parameter values are first used to calculate the model-predicted responses for all of the experiments in the data set.

A **parity plot** is constructed by plotting the experimental responses *vs*. the model-predicted responses as points. A diagonal parity line, corresponding to the the experimental responses being equal to the model-predicted responses is than added to the graph. The closer the points are to the parity line, the higher the accuracy of the rate expression.

To generate **a set of residuals plots**, an experiment residual is calculated for each data point. The experiment residual is the difference between the experimental response for a data point and the corresponding model-predicted response. Plotting the set of experiment residuals *vs.* each of the adjusted experimental inputs yields the set of residuals plots.

In general, the parity plot is used to gauge the accuracy of the model while the residuals plots are examined to see whether there are systematic trends in the residuals. The points in a residuals plot should scatter randomly about zero. If a systematic trend is observed, it may suggest that the adjusted input used to generate the plot has an effect upon the rate that is not being accuratly captured by the rate expression. Thus, in graphical assessment, the following criteria suggest that the model is accurate.

* The points in the parity plot are all close to a diagonal, parity line ($y_{expt} = y_{model}$).
* In each residuals plot, the points scatter randomly about zero (the horizontal axis), and no systematic deviations are apparent.

When same-temperature data blocks are analyzed separately using a linearized model, graphical assessment makes use of a **model plot** and an **Arrhenius plot** for each rate coefficient or equilibrium constant being estimated. In these graphs, the linearized model is plotted as a line and the linearized data are plotted as points. In these plots, the deviations of the data from the line will be small and random with no apparent trends in the deviations if the model is accurate.

### Deciding Whether to Accept the Proposed Rate Expression

The final step in kinetics data analysis involves **making a decision** to accept, reassess with additional data, or reject the proposed rate expression. In the end, this is a judgement call, and it becomes easier as a reaction engineer gains experience.  The intended use of the rate expression, and more importantly the potential consequences of accepting an inaccurate rate expression, should be given serious consideration when making this decision. That is, if accepting an inaccurate rate expression might result in severe personal injury, significant property damage or catastrophic financial loss, the rate expression should be very, very accurate if it is accepted. If the consequences of accepting an inaccurate rate expression are less severe, somewhat lower accuracy may be deemed acceptable.

## Examples

This chapter described the generation and analysis of kinetics data. The analysis of kinetics data was described in general terms. That information will be applied to the analysis of data from BSTRs, CSTRs and PFRs and examples will be presented in the next three chapters. The examples presented here can be applied to data generation and analysis using any of the ideal reactor types. [Example -@sec-example_18_6_1] illustrates the design of kinetics experiments for the purpose of generating kinetics data. [Example -@sec-example_18_6_2] shows how to modify the model used by a numerical fitting function to estimate the base-10 logarithm of a pre-exponential factor, and how to process the results returned by the numerical fitting function.

### Design of Kinetics Experiments {#sec-example_18_6_1}

{{< include examples/reb_18_6_1/narrative.qmd >}}

---

:::{.callout-tip collapse="true"}
## Click Here to See What an Expert Might be Thinking at this Point

I am asked to design kinetics experiments, so I need to decide which variables will be adjusted in the experiments. I further need to decide how many levels to use for each adjusted variable and what those levels should be. The problem states that the rate expression developed using these data will be used to design a new process, so I want to be sure to generate a sizeable data set that spans the expected ranges of the adjusted variables and also captures the effects of each of them upon the response. Based upon the information presented in the problem statement, the experimental response here will be the concentration of A.

Reaction rates can be affected by the temperature and the concentration of each reagent present. Here the problem states that the rate is not affected by the concentration of Z, so the temperature and the concentration of A should vary from one experiment to the next. The reactor is isothermal, so the temperature can be adjusted directly in the experiments. The concentration of A will change as the reaction proceeds. This suggests two ways to vary the concentration of A from experiment to experiment. The first is to adjust the initial concentration of A and the second is to adjust the time at which the response is measured. Longer times will lead to smaller concentrations of A because at longer times more of the A will have reacted. I will use both the initial concentration of A and the reaction time as adjusted variables.

Next I need to decide how many levels to use for each adjusted variable and what those levels should be. Normally I would use levels that span a slightly wider range than the range where the rate expression will be used. Here, however, I'm told that the rate is too low below 65 °C and undesirable reactions occur above 90 °C, so I will choose levels that just span that range. The range only spans 25 °C, so four temperature levels seem reasonable, as does spacing them equally across the range.

I want to span a range of concentrations that is slightly wider than the expected range where the rate expression will be used. Three initial concentration levels of 0.5, 1.0, and 1.5 M will do so. Then, in order to ensure that the data are sensitive to the effect of the concentration of A, I will use six levels of reaction time. Noting that at 80 °C it takes 30 min for the reaction to go to completion, spacing the reaction times 5 minutes apart will lead to samples that span a wide range of conversions, and hence a wide range of concentrations of reagent A.

:::

**Experimental Design**

Three reactor inputs will be adjusted in the experiments: the temperature, $T$, the initial concentration of A, $C_{A,0}$, and the reaction time, $t$. The temperature levels will be 65, 73, 82, and 90 °C; the initial concentration levels will be 0.5, 1.0, and 1.5 M; the reaction time levels will be 5, 10, 15, 20, 25, and 30 min. All possible combinations of these levels will be studied giving a total of 72 experimental data points.

It will not be necessary to perform 72 experiments, however. Using a reactor at 65 °C with an initial concentration of A equal to 0.5 M, six responses can be recorded in the experiment (at reaction times of 5, 10, 15, 20, 25, and 30 min). The number of experiments needed to record all 72 responses is 12. The initial conditions for those 12 experiments are shown in @tbl-example_18_6_1. Each experiment in the table will yield responses at all six reaction time levels.

| Experiment | T (°C) | C~A,0~ (M) |
|:------:|:-------:|:----------:|
| 1 | 65 | 0.5 |
| 2 | 65 | 1.0 |
| 3 | 65 | 1.5 |
| 4 | 73 | 0.5 |
| 5 | 73 | 1.0 |
| 6 | 73 | 1.5 |
| 7 | 82 | 0.5 |
| 8 | 82 | 1.0 |
| 9 | 82 | 1.5 |
| 10 | 90 | 0.5 |
| 11 | 90 | 1.0 |
| 12 | 90 | 1.5 |

: Initial conditions for kinetics experiments. {#tbl-example_18_6_1 tbl-colwidths="[30,35,35]"}

:::{.callout-note collapse="false"}
## Note

Some readers might have difficulty with this problem because it relies upon an understanding of how BSTR kinetics experiments are performed and the data they generate, and that information has not been presented yet. BSTR kinetics experiments are considered in @sec-6_bstr_data_analysis. The important aspects of this example are that it was first necessary to decide which reactor model input variables to adjust, and then it was necessary to decide how many levels of each variable to use and what those levels should be. After doing that, the experimental design simply involves performing an experiment at every combination of the adjusted variable levels.

:::

### Estimating the Base-10 Logarithm of a Pre-Exponential Factor {#sec-example_18_6_2}

A reaction engineer has been tasked with fitting the model shown in equation (1) to a set of experimental data. In that model, $\dot{n}_{A,f}$ is the response, $C_{A,0}$ and $\dot{V}$ are the adjusted experimental inputs, $V$ and $R$ are known constants, and $k_0$ and $E$ are the rate expression parameters that must be estimated. When the engineer attempted to use a numerical fitting function to complete the task, it failed to converge, yielding unacceptably poor parameter estimates. How can the model be modified to improve the likelihood of obtaining good parameter estimates, and how does the modification affect the use of the numerical fitting function?

$$
0 = \dot{V}C_{A,0} - \dot{n}_{A,f} - k_0 \exp{\left( \frac{-E}{RT} \right)}\frac{\dot{n}_{A,f}}{\dot{V}} \tag{1}
$$

---

The problem with using a numerical fitting function to estimate $k_0$ and $E$ in the given model is that a guess must be provided for $k_0$. The actual value of $k_0$ could be anywhere in the range from 10^-20^ to 10^20^ or wider, making extremely difficult to make an ititial guess.

The situation will be improved by defining $\beta$ as shown in equation (2), and re-writing the model as shown in equation (3).

$$
\beta = \log_{10} \left(k_0\right) \tag{2}
$$

$$
0 = \dot{V}C_{A,0} - \dot{n}_{A,f} - 10^\beta \exp{\left( \frac{-E}{RT} \right)}\frac{\dot{n}_{A,f}}{\dot{V}} \tag{3}
$$

The numerical fitting function now can be used to estimate $\beta$ instead of $k_0$. The range of possible values of $\beta$ is between -20 and +20. This makes it easier to make a guess, and it makes it more likely that when the numerical fitting function will be able to estimate $\beta$.

Assuming the numerical fitting function is successful, it will return the best estimate for $\beta$. The best estimate for $k_0$ can be found using equation (4).

$$
\beta = \log_{10} \left(k_0\right) \qquad \Rightarrow \qquad k_0 = 10^\beta \tag{4}
$$

The numerical fitting function will also return a measure of the uncertainty in $\beta$. If it returns the upper and lower limits of the 95% confidence interval for $\beta$, denoted as $\beta_{CI,l}$ and $\beta_{CI,u}$ the upper and lower limits of the confidence interval for $k_0$, $k_{0,CI,l}$ and $k_{0,CI,u}$, can be calculated using equations (5) and (6). If the numerical fitting function returns the stardard error for $\beta$, $\lambda_\beta$, the standard error for $k_0$, $\lambda_{k_0}$ can be calculated using equation (7).

$$
k_{0,CI,l} = 10^{\beta_{CI,l}} \tag{5}
$$

$$
k_{0,CI,u} = 10^{\beta_{CI,u}} \tag{6}
$$

$$
\lambda_{k_0} = k_0 \ln \left(10\right) \lambda_\beta \tag{7}
$$

## Symbols Used in @sec-6_kin_data_gen

| Symbol | Meaning |
|:-------|:--------|
| $k$ | Rate coefficient, an additional index is used to denote the reaction if more than one reaction is taking place. |
| $k_0$ | Pre-exponential factor in the Arrhenius expression. |
| $n_i$ | Molar amount of reagent $i$; an additional subscript denotes the time. |
| $\dot{n}_i$ | Molar flow rate of reagent $i$; an additional subscript denotes the location. |
| $C_i$ | Concentration of reagent $i$; an additional subscript denotes the time or location. |
| $E$ | Activation energy in the Arrhenius expression. |
| $P$ | Pressure. |
| $R$ | Ideal gas constant. |
| $R^2$ | Coefficient of determination. |
| $T$ | Temperature. |
| $V$ | Volume. |
| $\dot{V}$ | Volumetric flow rate. |

: {tbl-colwidths="[20,80]"}