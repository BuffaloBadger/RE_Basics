# Parameter Estimation {#sec-apndx_parameter_est}

This appendix provides a simplified overview on using experimental data to (a) estimate the values of parameters in a model and (b) assess the accuracy of the resulting model. The special case of linear models, and particularly the Arrhenius expression is presented first. A general parameter estimation procedure that can be applied to any model follows that. In both cases, only single response data are considered. The presentation here is not specific to any one computer program or language. The information provided here should be sufficient to understand the examples presented in *Reaction Engineering Basics* and to perform parameter estimation using software of one's choosing. Readers seeking a more complete understanding should consult a statistical analysis textbook or take a course on statistical analysis.

## Defining the Problem

Parameter estimation involves using experimental data to find the best values for unknown, constant parameters appearing in a model of those experiments. This is sometimes called fitting the model to the data. In each experiment, the values of one or more *adjusted input variables* are set, and the value of an *experimental response* is measured. In *Reaction Engineering Basics* only experiments with a single experimental response are considered. Every experiment that is performed involves the same set of adjusted input variables and the same experimental response, but their values are different from experiment to experiment.

**Estimating the Parameter Values**

Given values for the unknown model parameters, the model can be used to calculate the *model-predicted response* for every experiment. The difference between the experimental response and the model predicted response is the error, or residual, for that experiment. The "best" values for the unknown model parameters are taken to be the ones that minimize the sum of the squares of those errors. The squares of the errors are used instead of the errors themselves so that positive and negative errors don't cancel each other out. For this reason, parameter estimation is sometimes referred to as "least-squares fitting." An alternative choice for the "best" parameter values uses the sum of the squares of the *relative* errors. The use of this definition is considered later in this appendix.

The resulting values of the paremeters are estimates. Their values depend upon the experimental data used to calculate them. If the exact same set of experiments was performed two times using the exact same experimental equipment, the resulting data sets would *not* be identical. There is random error associated with any experimental measurement, and for that reason there would be small differences between the two data sets, even if there were no other sources of error. The "best" values for the model parameters calculated using one of the data sets would not be exactly equal to the values found using the other data set. This is why the process is called parameter estimation.

**Statistical Analysis**

Additional statistical calculations are typically performed when estimating the parameter values. These calculations make certain assumptions about the data, for example that the errors conform to a normal distribution. To gauge the uncertainty in the estimated value of each parameter, the upper and lower limits of its 95% confidence interval, $CI_u$ and $CI_l$, can be calculated. Recall that repeating the experiments would result in a slightly different data set, and therefore a slightly different estimated parameter value. If the experiments were repeated 100 times, the estimated parameter value would fall within this interval 95 times. An alternative way to gauge the uncertainty in the estimated value of each parameter is to calculate its standard error, $\lambda$.

It is very common to calculate the coefficient of determination, $R^2$, following parameter estimation. While the 95% confidence interval and the standard error are measures of uncertainty in each estimated parameter, the coefficient of determination provides an indication of the accuracy of the entire model. $R^2$ is the proportion of the variation in the model-predicted response that is predictable from the adjusted input variables. As such, the model is perfectly accurate when $R^2$ is equal to 1.0, and the accuracy decreases as $R^2$ becomes smaller.

So to summarize, parameter estimation is a process where a model is fit to experimental data by minimzing the sum of the squares of the errors between the experimental and predicted responsees. Software that does this is referred to herein as a "fitting function." Typically a fitting function additionally calculates the coefficient of determination and a measure of the uncertainty in the estimated parameters (e. g. their standard errors or their 95% confidence intervals).

## Fitting Functions for a Linear Model

The parameters in a model are estimated by minimizing the sum of the squares of the errors, $\Psi$, between the experimental responses, $y_i$, and the model-predicted responses, $\hat{y}_i$, @eq-sum_of_squares_of_errors. If there is only one adjusted input variable, $x$, and the model is linear, $\hat{y}_i = mx_i + b$, the sum of the squares of the errors is given by @eq-sum_of_squares_linear, and the minimization can be performed analytically.

$$
\Psi = \sum_i \left( \left( y_i - \hat{y}_i \right)^2 \right)
$${#eq-sum_of_squares_of_errors}

$$
\Psi = \sum_i \left( \left( y_i - mx_i - b\right)^2 \right)
$${#eq-sum_of_squares_linear}

The parameters for a linear model are $m$ and $b$. The values of $m$ and $b$ that minimize $\Psi$ can be found by setting the derivative of $\Psi$ from @eq-sum_of_squares_linear with respect to each of the parameters equal to zero. Those equations can be solved algebraically to get analytical expressions for the best values of $m$ and $b$, @eq-linear_least_squares_parameters where $N$ is the number of experiments and the summations are over all experiments. 

$$
\begin{matrix}\displaystyle \frac{\partial \Psi}{\partial m} = 0 \\ \\ \displaystyle \frac{\partial \Psi}{\partial b} = 0 \end{matrix} \qquad \Rightarrow \qquad \begin{matrix} m = \displaystyle \frac{N \sum \left(x_iy_i\right) - \left(\sum x_i \right)\left(\sum y_i \right)}{N \sum\left(x_i^2\right) -\left( \sum x \right)^2}\\ \\ b = \displaystyle \frac{\sum\left(y_i\right) - m \sum\left(x_i\right)}{N} \end{matrix}
$${#eq-linear_least_squares_parameters}

The best values of the parameters in any analytic model can be found in this way, and since linear models are very common, almost any mathematics software package or spreadsheet will include a function that calculates the best slope and intercept from experimental $x-y$ data. In most cases the coefficient of determination, $R^2$, and measures of the uncertainty in the parameter values ($\lambda_m$ and $\lambda_b$ or $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$) also can be calculated. 

When implemented in spreadsheets, a **model plot** is typically generated. Otherwise one can be created by plotting the experimental response, $y$, *vs*. $x$ as points and, on the same axes, the model-predicted response, $\hat{y}$, *vs*. $x$ as a line. The model plot can be used to assess the accuracy of the model. When the deviations of the experimental response points from the predicted response line are small, and there are no trends in the deviations, the model is accurate.

Irrespective of the specific software package being used, a linear model fitting function must be povided with the adjusted input variables and the corresponding experimental responses for all experiments. At the minimum, a linear model fitting function will calculate and return the estimated slope, $m$, and intercept, $b$. With most software packages the linear model fitting function or associated auxillary functions will additionally calculate and return the coefficient of determination, $R^2$, and either the standard errors for the estimated paramerers, $\lambda_m$ and $\lambda_b$, or the upper and lower limits of their 95% confidence intervals, $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,b}$.

### Estimation of Arrhenius Expression Parameters

The estimation of the parameters in the Arrhenius expression, namely the pre-exponential factor and the activation energy, is a routine task that is performed often in the analysis of kinetics data. As such, it can make sense to write a dedicated function for estimating Arrhenius expression parameters.

@sec-2_rates_rate_express showed that by taking the logarithm of both sides of the Arrhenius expression, it can be converted to the linear form given in @eq-linear_arrhenius and reproduced below. @eq-x_and_y_in_linear_arrhenius shows that by defining $y = \ln{k_j}$ and $x = \frac{-1}{RT}$, the equations indeed is linear with a slope equal to $E_j$ and an intercept equal to $\ln{k_{0,j}}$

$$
\ln{k_j} = E_j \left( \frac{-1}{RT} \right) + \ln{k_{0,j}}
$$

$$
\begin{matrix} y = \ln{k_j} \\ x = \displaystyle \frac{-1}{RT} \end{matrix} \quad \Rightarrow \quad y = m x + b \quad \Rightarrow \quad \begin{matrix} m = E_j \\ b = \ln{k_{0,j}} \end{matrix}
$${#eq-x_and_y_in_linear_arrhenius}

Given a set of temperatures in absolute units, a corresponding set of rate coefficient values, and the ideal gas constant, $x$ and $y$ can be calculated for each data pair as shown in @eq-x_and_y_in_linear_arrhenius. Notice that the temperatures and the ideal gas constant must have the same *absolute* temperature units. Also, the energy units for the activation energy will be the energy units of the ideal gas constant.

The resulting set of $x$ values and the corresponding set of $y$ values can be passed to a linear model fitting function of one's choosing. The linear model fitting function will return $m$, $b$, $R^2$, and either $\lambda_m$ and $\lambda_b$, or $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$.

As @eq-x_and_y_in_linear_arrhenius indicates, the activation energy is equal to the slope, so the actvation energy and its uncertainty (as its standard error or its 95% confidence interval) are given Equations [-@eq-E_linear_least_squares], [-@eq-E_lambda_linear_least_squares], and [-@eq-E_CI_linear_least_squares].

$$
E_j = m
$$ {#eq-E_linear_least_squares}

$$
\lambda_{E_j} = \lambda_m
$$ {#eq-E_lambda_linear_least_squares}

$$
\begin{align}
CI_{u,E_j} &= CI_{u,m} \\
CI_{l,E_j} &= CI_{l,m}
\end{align}
$$ {#eq-E_CI_linear_least_squares}

@eq-x_and_y_in_linear_arrhenius shows that the intercept, $b$, is the logarithm of the pre-exponential factor, so equations [-@eq-k0_linear_least_squares], [-@eq-k0_lambda_linear_least_squares], and [-@eq-k0_CI_linear_least_squares] must be used to calculate $k_{0,j}$ and its uncertainty.

$$
k_{0,j} = \exp {\left(b\right)}
$${#eq-k0_linear_least_squares}

$$
\lambda_{k_{0,j}} = k_{0,j} \exp {\left(\lambda_b\right)}
$${#eq-k0_lambda_linear_least_squares}

$$
\begin{aligned}
CI_{u,k_{0,j}} &= \exp{\left( CI_{u,b} \right)} \\
CI_{l,k_{0,j}} &= \exp{\left( CI_{l,b} \right)}
\end{aligned}
$${#eq-k0_CI_linear_least_squares}

A dedicated function for estimating Arrhenius expression parameters then can be structured as follows.

1. It should receive a vector of $k$ values, a corresponding vector of $T$ values (in absolute units), and the value of the ideal gas constant, $R$, in the same temperature units and the energy units desired for the activation energy as arguments
2. It should calculate vectors of $x$ and $y$ using equation @eq-x_and_y_in_linear_arrhenius.
3. It should calculate $m$, $b$, $R^2$, and either $\lambda_m$ and $\lambda_b$ or  $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$ by calling a linear model fitting function providing the $x$ and $y$ vectors.
4. It should calculate $k_{0,j}$, $E_j$, and either $\lambda_{k_{0,j}}$ and $\lambda_{E_j}$ or $CI_{u,k_{0,j}}$, $CI_{l,k_{0,j}}$, $CI_{u,E_j}$, and $CI_{l,E_j}$ using Equations [-@eq-E_linear_least_squares], [-@eq-E_lambda_linear_least_squares], [-@eq-E_CI_linear_least_squares], [-@eq-k0_linear_least_squares], [-@eq-k0_lambda_linear_least_squares], and [-@eq-k0_CI_linear_least_squares].
5. It should return $k_{0,j}$, $E_j$, $R^2$, and either $\lambda_{k_{0,j}}$ and $\lambda_{E_j}$ or $CI_{u,k_{0,j}}$, $CI_{l,k_{0,j}}$, $CI_{u,E_j}$, and $CI_{l,E_j}$.

## Fitting Functions for a Numerical Model

In *Reaction Engineering Basics*, reactor models are solved numerically. Therefore a fitting function that can use a numerical model must be used for parameter estimation. Many software packages include this kind of fitting function. An overview of the use of fitting functions for numerical models is presented in @sec-6_kin_data_gen. The flow of information is diagram from that discussion is reproduced here as @fig-apndx_L_param_est_info_flow. This section offers an abbreviated description of how fitting functions work.

![Information flow for fitting a reactor model to an experimental data set using a computer fitting function.](Graphics/parameter_estimation_info_flow.png){#fig-apndx_L_param_est_info_flow width="70%"}

The parameters are still estimated by minimizing the sum of the squares of the errors, @eq-sum_of_squares_of_errors, but the minimization is performed numerically. Regardless of the specific software being used, four things must be provided to the fitting function, as indicated in @fig-apndx_L_param_est_info_flow. Those inputs to the fitting function are

1. An initial guess for the values of the model parameters.
2. The values of the adjusted input variables (factos) for all of the experiments being analyzed.
3. The corresponding values of the experimental responses.
4. A responses function, written by the user, that uses the model to calculate and return the corresponding model-predicted responses.
    a. The responses function receives a guess for the model parameters and the values of adjusted input variables for all of the experiments and it returns the model-predicted responses for all of the experiments.

A fitting function works in much the same manner as an ATE solver (see @sec-apndx_solve_ates) except that instead of finding values of unknowns that cause a set of residuals to equal zero it finds values of model parameters that minimize the sum of the squares of the errors. In essence, the fitting function finds the best parameter values by trial and error.

* It creates variables to hold the best parameter estimates and the corresponding best sum of the squares of the errors.
* It calls the responses function to get the model predicted responses and then calculates $\Psi$ using @eq-sum_of_squares_of_errors.
* It saves the initial guess and the corresponding $\Psi$ as the best values.
* It repeatedly 
    * generates an improved guess 
    * calculates $\Psi$ as above, and 
    * if $\Psi$ is less than the best $\Psi$ it saves the improved guess and corresponding $\Psi$ as the best values.
* It stops generating new guesses when it is unable to generate an improved guess that results in a smaller $\Psi$.
* It calculates $R^2$, and either the standard errors or the 95% confidence interval for each parameter.
* It returns the estimated parameter values, $R^2$, and either the standard errors or the 95% confidence interval for each parameter.

### Convergence

Parameter estimation as described above is an iterative process. Ideally, as the fitting function generates improved guesses, the corresponding sum of the squares of the errors gets smaller and smaller. This is called **convergence**. The fitting function uses a set of convergence criteria to determine whether (a) it has converged to a minimum value of the sum of the squares of the errors, (b) it has not converged to a minimum value of the sum of the squares of the errors but it is making progress and should continue iterating or (c) it has not converged to a minimum value and it is not making progress, so it should stop iterating. Typical convergence criteria are listed below; the fitting function usually provides a default for the "specified amounts" in this list, but the defaults can be overridden at the time the fitting function is called. Along with the estimated parameters, the coefficient of determination, and the parameter uncertainties, the fitting function should return a flag or message that indicates whether it converged and why it stopped iterating.

* the sum of the squares of the errors is smaller than a specified amount (converged).
* the sum of the squares of the errors is smaller than it was for the previous iteration (making progress)
* a (large) specified number of iterations has occurred (not making progress).
* the sum of the squares of the errors have been calculated a specified number of times (not making progress).
* the sum of the squares of the errors is increasing instead of decreasing (not making progress).
* the sum of the squares of the errors is changing by less than a specified amount between iterations (not making progress).
* the guess is changing by less than a specified amount between iterations (not making progress).

### Caveats

A few issues should be kept in mind when performing numerical parameter estimation. The first is that **the fitting function may converge to a local minimum** of the sum of the squares of the errors and not the global minimum. This could result in a model that does not appear to be accurate, but that could be accurate if the global minimum was located. One way to try to detect this situation is to repeat the parameter estimation using a very different initial guess. If the solver converges to a different set of estimated parameters, that indicates that one (or possibly both) of the sets of estimated parameters corresponds to a local minimum. If a wide range of initial guesses always leads to the same parameter estimates, that *may* suggest the a global minimum has been found.

The second issue arises **when initially guessing the value of a model parameter that might fall anywhere in a very wide range of values**. As an example, in *Reaction Engineering Basics* problems, the pre-exponential factor for a rate coefficient could have a value anywhere between 10^-20^ and 10^20^. In this situation, if the initial guess is not sufficiently close to the actual value of the parameter, the fitting function may quit because it is not making progress. One way to reduce the likelihood of this happening is to use the base-10 logarithm of the parameter instead of the parameter itself. That is, if $k_0$ is the actual parameter of interest in the model, re-write the model replacing $k_0$ with $10^\beta$. Then perform parameter estimation to find the best value of $\beta$. When the possible range of $k_0$ is between 10^-20^ and 10^20^, the corresponding range of $\beta$ is between -20 and 20. Once the best value $\beta$ has been found, the best value of $k_0$ is simply calculated as $k_0 = 10^\beta$. 

The third issue arises **when the experimental responses span several orders of magnitude**. In this situation, the errors associated with larger values of the response may be greater than errors associated with smaller values of the response. If that is so, the minimization process will focus on the responses with larger values because they have a greater effect upon the sum of the squares of the errors. One option for addressing this situation is to minimize the sum of the squares of the *relative* errors instead of the sum of the squares of the *absolute* errors.

## Assessing Model Accuracy

Typically, if successful, the solver will return (a) the estimated values of the model parameters, (b) some measure of the uncertainty in the estimated values (e. g. a 95% confidence interval) and (c) the coefficient of determination, $R^2$. However, finding the best values for the parameters does not ensure that the model is accurate (see @fig-line_fit_to_parabola).

When the model being fit to the data is linear and has only one adjusted input variable a model plot can be created as described above. Otherwise, the estimated values of the model parameters can be used to compute the model-predicted response for each experiment. A **parity plot** can then be created wherein the model-predicted response is plotted versus the experimental response. If the model was perfect, every point in the parity plot would fall on a diagonal line passing through the origin. The farther the points are from the diagonal, the lower the accuracy of the model.

The errors, or residuals, for each experiment can also be calculated. A set of **residuals plots** can then be created wherein the residuals are plotted versus each of the experimental adjusted input variables. If the model was perfect, every residual would equal zero and every point in every residuals plot would fall on the horizontal axis. The magnitude of the deviations from the horizontal axis is not important (the parity plot is used for this purpose). The critical issue whether the points scatter randomly above and below the horizontal axis as the value of the adjusted input variable increases. When the model is good, there should not be any trends in the deviations as the value of the adjusted input variable increases.

Deciding whether the model is sufficiently accurate is a judgement call. The following criteria are satisfied by an accurate model.

* The coefficient of determination, $R^2$, is nearly equal to 1.
* The uncertainty in most, if not all, of model parameters is small compared to the parameter's value.
    * When using standard errors of the parameters, they are small compared to the parameter value.
    * When using 95% confidence intervals, the upper and lower limits of the interval are close to the parameter value.
* The points in the parity plot are all close to a diagonal line passing through the origin.
* In each residuals plot, the points scatter randomly about the horizontal axis, and no systematic deviations are apparent.

As noted in @sec-6_kin_data_gen, the uncertainty in *most* of the parameters might be small, but there could be a few for which the uncertainty is large. This could indicate one of three possibilities. First, the factor levels (values to which the adjusted input variables were set) used in the experiments may not allow accurate resolution of those parameters that have high uncertainty. Second, the parameters with higher uncertainty may be mathematically coupled to each other (e. g. the model-predicted response may only depend on the product of two parameters so that the individual parameters can have any values as long as their product has the optimum value). Alternatively, the parameters with high uncertainty may not be needed, and there may be a simpler model with fewer parameters that is equally accurate.

## Mathematical Formulation of Parameter Estimation

In *Reaction Engineering Basics* the unknown parameters typically will be constants that appear in the rate expression. They will always be calculated by calling a fitting function, and a responses function will always need to be written and provided to that fitting function. Generally the responses function will loop through the experiments. For each experiment it will solve the reactor design equations for the experimental reactor and use the results to calculate the model-predicted response. The typical steps in formulating parameter estimation are as follow.

* Identify all known constants and their values.
* Formulate a reactor model for the experimental reactor assuming the rate expression parameters and the adjusted input variables will be available.
* Write any ancillary equations that are needed to solve the reactor model equations.
* Formulate a responses model that solves the reactor model equations.
* Write any ancillary equations that are needed to calculate the model-predicted response given the solution of the reactor model equations.

## Numerical Implementation of Parameter Estimation

Numerical implementation of parameter estimation entails writing code that performs the calculations described in the mathematical formulation above. The code will be written for a software package of the user's choosing and will utilize a fitting function provided by that software package. The specific structure of the code will depend upon the software package being used and programmer preference. Here it is assumed that the code will be broken into at least four computer functions that each perform a specific task.

reactor model function
: receives a value for each adjusted input variable and values of the parameters being estimated, calls an appropriate solver to solve the reactor design equations, checks that the solver was successful, and returns the results that the solver returned.

derivatives function and/or residuals function
: evaluates the derivatives or the residuals for the reactor design equations as described in Appendices [-@sec-apndx_solve_ates] and [-@sec-apndx_solve_ivodes].

responses function
: receives values of the parameters being estimated and the full set of adjusted input variables, uses the reactor model function to solve the reactor design equations for each experiment, uses the results from solving the reactor design equations to calculate the model-predicted response for each experiment, returns the full set of model-predicted responses.

top level code (and/or a master calculations function)
: sets all given and known constants, establishes a mechanism for making quantities available to all functions as necessary, reads and stores the experimental data set (adjusted input variables and experimental responses), calls the fitting function to get the parameter estimates, parameter uncertainties, and coefficient of determination, performs any additional processing such as making a model plot or making parity and residuals plots, and reports or saves the results.

## Examples

```{r}
#| echo: false
#| output: false
library(tidyverse)
library(knitr)
library(kableExtra)
source('~/Libraries/R/fmt_tibble_col.R')
source('~/Libraries/R/fmt_E_to_10.R')
```

This appendix includes two examples. [Example -@sec-example_L_7_1] illustrates the use of linear least squares to calculate the pre-exponential factor and the activation energy in the Arrhenius expression. The numerical implementation is structured such that one of the functions could be used extracted and used repeatedly as a utility function whenever it is necessary to estimate the parameters in the Arrhenius expression. Example [-@sec-example_L_7_2] illustrates parameter estimation using a numerical reactor model.

### Arrhenius Expression Parameter Estimation {#sec-example_L_7_1}

{{< include examples/reb_L_7_1/narrative.qmd >}}

For convenience, the Assignment Summary and Mathematical Formulation of the Analysis from [Example -@sec-example_4_5_4] are reproduced here without the expert thinking callouts.

#### Assignment Summary

**Items of Interest:** $k_0$, $E$, $k_{0,CI,u}$, $k_{0,CI,l}$, $E_{CI,u}$, $E_{CI,l}$, and an assessment of the accuracy of equation (1)

$$
k = k_0 \exp{ \left( \frac{-E}{RT} \right)} \tag{1}
$$

**Known Constant:** $R$ = 8.314 J mol^-1^ K^-1^.

**Experimental Data:** $\underline{T_C}$ and $\underline{k_{\text{expt}}}$.

#### Mathematical Formulation of the Analysis

**Rate Coefficient Model**:

[Linearized Arrhenius Expression]{.underline}

$$
\ln{\left(k \right)} = E \left( \frac{-1}{RT} \right) + \ln{\left(k_0 \right)} \tag{2}
$$

$$
\begin{matrix} y = \ln{\left(k \right)} \\ x = \displaystyle \frac{-1}{RT} \end{matrix} \quad \Rightarrow \quad y = m x + b \quad \Rightarrow \quad \begin{matrix} m = E \\ b = \ln{\left(k_0 \right)} \end{matrix} \tag{3}
$$

**Parameter Estimates and Statistics**

The best estimates for $k_0$, $E$, their 95% confidence intervals, and the coefficient of determination, $R^2$, can be found using the following sequence of calculations.

$$
\underline{k_{\text{expt}}} \qquad \Rightarrow \qquad y_{\text{expt}} = \ln{\left(k_{\text{expt}}\right)} \qquad \Rightarrow \qquad \underline{y_{\text{expt}}} \tag{4}
$$

$$
\underline{T_C} \qquad \Rightarrow \qquad x = \left( \frac{-1}{R\left(T_C + 273.15\right)} \right) \qquad \Rightarrow \qquad \underline{x} \tag{5}
$$

$$
\begin{matrix} \underline{x} \\ \underline{y_{\text{expt}}} \end{matrix}  \qquad \begin{matrix} \text{linear model} \\ \Rightarrow \\ \text{fitting function} \end{matrix} \qquad \begin{matrix} m, m_{CI,u}, m_{CI,l} \\ b, b_{CI,u}, b_{CI,l} \\ R^2 \end{matrix} \tag{6}
$$

$$
E = m \tag{7}
$$

$$
E_{CI,u} = m_{CI,u} \tag{8}
$$

$$
E_{CI,l} = m_{CI,l} \tag{9}
$$
$$
b = \ln{k_0} \qquad \Rightarrow \qquad k_0 = \exp{\left(b\right)} \tag{10}
$$

$$
k_{0,CI,u} = \exp{\left(b_{CI,u}\right)} \tag{11}
$$

$$
k_{0,CI,l} = \exp{\left(b_{CI,l}\right)} \tag{12}
$$

**Model Plot**

Using the estimated values of $k_0$ and $E$ and the experimental temperature data, $\underline{T_C}$, the model-predicted responses, $\underline{k_{\text{model}}}$ can be calculated as shown below.

$$
\underline{T_C} \quad \Rightarrow \quad k_{\text{model}} = k_0 \exp{ \left( \frac{-E}{R\left(T_C + 273.15\right)} \right)} \quad \Rightarrow \quad \underline{k_{\text{model}}} \tag{13}
$$

A model plot can then be created by plotting $k_{\text{expt}}$ *vs*. $\frac{1}{T_C + 273.15}$ as points and $k_{model}$ *vs*. $\frac{1}{T_C + 273.15}$ as a line on the same axes.

**Performing the Analysis**

The calculations can be completed as follows:

1. Calculate the parameter estimates and statistics, $k_0$, $E$, $k_{0,CI,u}$, $k_{0,CI,l}$, $E_{CI,u}$, $E_{CI,l}$, and $R^2$, as described above.
2. Generate a model plot as described above.

---

#### Numerical Implementation of the Solution

The computer code that performs the calculations could be structured in a number of ways. @sec-apndx_notation summarizes the coding structure that is assumed in *Reaction Engineering Basics*. Accordingly, the numerical implementation involves creating a single computer function for this assignment and doing the following within that function:

1. Write an Arrhenius parameters function that
    a. Receives the $\underline{k_{\text{expt}}}$ and $\underline{T}$ data sets and the ideal gas constant, $R$ as arguments,
        i. The temperatures must be in absolute (K or °R) units,
        ii. The gas constant must be in the same temperature units as the $\underline{T}$ data set,
        iii. The energy and mole units of the estimated activation energy will be the same as the energy and mole units of the ideal gas constant.
    b. Calculates the parameter estimates and statistics, as described in the mathematical formulation above,
        i. Since the temperatures are received in absolute units, it isn't necessary to convert the temperatures from °C.
    c. Returns $k_0$, $E$, $k_{0,CI,u}$, $k_{0,CI,l}$, $E_{CI,u}$, $E_{CI,l}$, and $R^2$.
2. Write an analysis function that
    a. Sets the value of $R$.
    b. Reads the $\underline{k_{\text{expt}}}$ and $\underline{T_C}$ data sets from the .csv file.
    c. Converts the $\underline{T_C}$ data set to a $\underline{T}$ data set where the temperatures are in K.
    d. Calculates $k_0$, $E$, $k_{0,CI,u}$, $k_{0,CI,l}$, $E_{CI,u}$, $E_{CI,l}$, and $R^2$ by calling the Arrhenius parameters function from step 1 passing the $R$, $\underline{k_{\text{expt}}}$ and $\underline{T}$ as arguments.
    e. Generates a model plot as described in the mathematical formulation above.
    f. Displays and/or saves the results to file.
3. Call the analysis function from step 2 above.

:::{.callout-note collapse="false"}
## Note

If the linear model fitting function used in the Arrhenius parameters function above returns the uncertainties as the standard errors of the estimated parameters, $\lambda_m$ and $\lambda_b$, instead of their 95% conficence intervals, then the standard errors in the estimated activation energy and pre-exponential factor must be calculated using equations (14) and (15).

$$
\lambda_{E_j} = \lambda_m \tag{11}
$$

$$
\lambda_{k_{0,j}} = k_{0,j} \exp {\left(\lambda_b\right)} \tag{12}
$$

:::

#### Results and Discussion

The numerical results were presented and discussed in [Example -@sec-example_4_5_4] and that won't be repeated here. The focus in this appendix is the numerical implementation of the calculations. The estimation of $k_0$ and $E$ and calculation of the associated statistics is a task that is performed almost every time kinetics data are analyzed. As such, the Arrhenius parameter function from this example can be extracted from the code for this particular example and made available like other equations solvers. By doing so, it can be called from within every *Reaction Engineering Basics* kinetics data analysis example that requires estimating Arrhenius parameters and statistics.

### Analysis of Kinetics Data from a BSTR {#sec-example_L_7_2}

{{< include examples/reb_L_7_2/narrative.qmd >}}

#### Assignment Summary

**Reactor System:** Isothermal, liquid-phase BSTR

**Reactor Schematic**

![Schematic Representation of the Experimental BSTR wherein the molar amounts vary with time while the temperature is constant.](Graphics/bstr_isothermal_schematic.png){#fig-example_L_7_2_schematic}

**Items of Interest:** $k$, accuracy of rate expression (1).

$$
r = kC_AC_B \tag{1}
$$

**Given and Known Constants:** $T$ = 70 °C, $C_{A,0}$ = 1.0 M, and $C_{B,0}$ = 0.9 M.

**Experimental Data:** $\underline{t_f}$, and $\underline{C_{Y,\text{expt}}}$.

**Basis:** $V$ = 1 L.

#### Mathematical Formulation of the Analysis

**BSTR Model**

[Design Equations]{.underline}

$$
\frac{dn_A}{dt} = - kVC_AC_B \tag{2}
$$

$$
\frac{dn_B}{dt} = - kVC_AC_B \tag{3}
$$

$$
\frac{dn_Y}{dt} = kVC_AC_B \tag{4}
$$

$$
\frac{dn_Z}{dt} = kVC_AC_B \tag{5}
$$

[Initial Values and Stopping Criterion]{.underline}

| Variable | Initial Value | Stopping Criterion |
|:------:|:-------:|:-------:|
| $t$ | $0$ | $t_f$ |
| $n_A$ | $n_{A,0}$ | |
| $n_B$ | $n_{B,0}$ | |
| $n_Y$ | $0$ | |
| $n_Z$ | $0$ | |
  
: Initial values and stopping criterion for solving equations (2) through (5). {#tbl-example_L_7_2_initial_values}

For each experiment the final time is known. The initial molar amounts of A and B can be calculated using equations (6) and (7).

$$
n_{A,0} = C_{A,0}V \tag{6}
$$

$$
n_{B,0} = C_{B,0}V \tag{7}
$$

[Derivatives Function]{.underline}

Given the values of $k$, $t$, $n_A$, $n_B$, $n_Y$, $n_Z$, and the quantities listed in the assignment summary the remaining quantities appearing in the derivatives expressions, $C_A$ and $C_B$ can be calculated using equations (8) and (9). Then the values of the derivatives can be calculated using equations (2) through (5).

$$
C_A = \frac{n_A}{V} \tag{8}
$$

$$
C_B = \frac{n_B}{V} \tag{9}
$$

[Solving the BSTR Design Equations]{.underline}

The BSTR design equations can be solved by calling an IVODE solver, providing it with the initial values, the stopping criterion, and a function that evaluates the derivatives as described above. The results returned by the solver should checked to make sure it was successful. Assuming it did succeed, the solver will return corresponding sets of values of $t$, $n_A$, $n_B$, $n_Y$, and $n_Z$ that span the range from $t=0$ to $t=t_f$.

**Responses Model**

Given a value for $k$ and the experimental data being analyzed, $\underline{t_f}$ and $\underline{C_{Y,\text{expt}}}$, the BSTR model can be solved as described above for each experiment in the data set. For each experiment, the model-predicted response, $C_{Y,\text{model}}$ can be calculated using equation (10). Combining the results yields $\underline{C_{Y,\text{model}}}$.

$$
C_{Y,\text{model}} = \frac{n_Y\big\vert_{t=t_f}}{V} \tag{10}
$$

---

:::{.callout-tip collapse="true"}
## Click Here to See What an Expert Might be Thinking at this Point

All I need to do to complete the mathematical formulation is to describe how to perform the analysis. I'll use a fitting function to calculate $k$, $k_{CI,u}$, $k_{CI_l}$, and $R^2$.

I need to assess the accuracy of the rate expression. Since there is only one paramter, $k$, in the model I can make a model plot to use in that assessment.

:::

**Performing the Analysis**

The analysis can be performed as follows.

1. Guess the value of $k$.
2. Call a fitting function that will calculate $k$, $k_{CI,u}$, $k_{CI_l}$, and $R^2$ using the responses model described above.
3. Use the estimated value of $k$ returned by the fitting function in the responses model above to calculate $\underline{C_{Y,\text{model}}}$.
4. Create a model plot with $\underline{C_{Y,\text{expt}}}$ *vs*. $\underline{t_f}$ as points and $\underline{C_{Y,\text{model}}}$ *vs*. $\underline{t_f}$ as a line.

#### Numerical Implementation of the Calculations

The computer code that performs the calculations could be structured in a number of ways. @sec-apndx_notation summarizes the coding structure that is assumed in *Reaction Engineering Basics*. Accordingly, the numerical implementation involves creating a single computer function for this assignment and doing the following within that function:

1. Make the given and known constants and the basis available within all functions.
2. Create a variable to hold the current value of $k$ and make it available within all functions.
3. Write a derivatives function that
    a. receives the independent and dependent variables, $t$, $n_A$, $n_B$, $n_Y$, $n_Z$, as arguments,
    b. evaluates the derivatives as described above for the BSTR model, and
    c. returns the values of $\frac{dn_A}{dt}$, $\frac{dn_B}{dt}$, $\frac{dn_Y}{dt}$, and $\frac{dn_Z}{dt}$.
4. Write a BSTR function that
    a. receives a value for $t_f$ as an argument,
    b. solves the BSTR design equations as described above for the BSTR model, and
    c. returns corresponding sets of values of $t$, $n_A$, $n_B$, $n_Y$, and $n_Z$ that span the range from $t=0$ to $t=t_f$.
5. Write a responses function that
    a. receives a value for $k$ and the set of adjusted input variables, $\underline{t_f}$, as arguments,
    b. makes $k$ available to all other functions, and
    c. calculates and returns $\underline{C_{Y,\text{model}}}$ as described above tor the responses model.
6. Write an analysis function that
    a. reads the experimental data, $\underline{t_f}$, and $\underline{C_{Y,\text{expt}}}$,
    b. performs the analysis as described at the end of the mathematical formulation above
        i. when calling the fitting function it should pass the initial guess for $k$, the experimental data, $\underline{t_f}$, and $\underline{C_{Y,\text{expt}}}$, and the name of the responses function in step 5
    c. displays the results.
7. Call the analysis function.

#### Results and Discussion

## Symbols Used in @sec-apndx_parameter_est

| Symbol | Meaning |
|:-------|:--------|
| $b$ | intercept of a linear function. |
| $k$ | Rate coefficient, a subscript is used to index the reactions if more than one are occurring. |
| $k_{0,}$ | Pre-exponential factor for a rate coefficient, an additional subscript is used to index the reactions if more than one are occurring. |
| $m$ | Slope in a linear function. |
| $x$ | Adjusted input variable, a subscript is used to index the experiments in an experimental data set. $i$ |
| $y$ | Experimental response, a subscript is used to index the responses in an experimental data set. |
| $\hat{y}$ | Model-predicted response, a subscript is used to index the responses in an experimental data set. |
| $CI$ | 95% confidence interval; when used as a subscript followed by "u" or "l" it indicates that the current variable is an upper or lower 95% confidence interval limit. |
| $E$ | Activation energy, a subscript is used to index the reactions if more than one are occurring. |
| $R$ | Ideal gas constant. |
| $R^2$ | Coefficient of determination. |
| $T$ | Temperature; a subscripted "C" denotes the units are Celsius. |
| $\lambda$ | Standard error in a parameter estimate, an additional subscript denotes the specific parameter. |
| $\Psi$ | Sum of the squares of the errors between the experimental response and the model-predicted response. |

: {tbl-colwidths="[20,80]"}
