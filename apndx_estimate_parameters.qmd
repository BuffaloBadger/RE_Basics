# Parameter Estimation {#sec-apndx_parameter_est}

This appendix provides a simplified overview on using experimental data to (a) estimate the values of parameters in a model and (b) assess the accuracy of the resulting model. The special case of linear models, and particularly the Arrhenius expression is presented first. A general parameter estimation procedure that can be applied to any model follows that. In both cases, only single response data are considered. The presentation here is not specific to any one computer program or language. The information provided here should be sufficient to understand the examples presented in *Reaction Engineering Basics* and to perform parameter estimation using software of one's choosing. Readers seeking a more complete understanding should consult a statistical analysis textbook or take a course on statistical analysis.

## Defining the Problem

Parameter estimation involves using experimental data to find the best values for unknown, constant parameters appearing in a model of those experiments. This is sometimes called fitting the model to the data. In each experiment, the values of one or more *adjusted input variables* are set, and the value of an *experimental response* is measured. In *Reaction Engineering Basics* only experiments with a single experimental response are considered. Every experiment that is performed involves the same set of adjusted input variables and the same experimental response, but their values are different from experiment to experiment.

**Estimating the Parameter Values**

Given values for the unknown model parameters, the model can be used to calculate the *model-predicted response* for every experiment. The difference between the experimental response and the model predicted response is the error, or residual, for that experiment. The "best" values for the unknown model parameters are taken to be the ones that minimize the sum of the squares of those errors. The squares of the errors are used instead of the errors themselves so that positive and negative errors don't cancel each other out. For this reason, parameter estimation is sometimes referred to as "least-squares fitting." An alternative choice for the "best" parameter values uses the sum of the squares of the *relative* errors. The use of this definition is considered later in this appendix.

The resulting values of the paremeters are estimates. Their values depend upon the experimental data used to calculate them. If the exact same set of experiments was performed two times using the exact same experimental equipment, the resulting data sets would *not* be identical. There is random error associated with any experimental measurement, and for that reason there would be small differences between the two data sets, even if there were no other sources of error. The "best" values for the model parameters calculated using one of the data sets would not be exactly equal to the values found using the other data set. This is why the process is called parameter estimation.

Additional statistical calculations are typically performed when estimating the parameter values. These calculations make certain assumptions about the data, for example that the errors conform to a normal distribution. To gauge the uncertainty in the estimated value of each parameter, the upper and lower limits of its 95% confidence interval, $CI_u$ and $CI_l$, can be calculated. Recall that repeating the experiments would result in a slightly different data set, and therefore a slightly different estimated parameter value. If the experiments were repeated 100 times, the estimated parameter value would fall within this interval 95 times. An alternative way to gauge the uncertainty in the estimated value of each parameter is to calculate its standard error, $\lambda$.

It is very common to calculate the coefficient of determination, $R^2$, following parameter estimation. While the 95% confidence interval and the standard error are measures of uncertainty in each estimated parameter, the coefficient of determination provides an indication of the accuracy of the entire model. $R^2$ is the proportion of the variation in the model-predicted response that is predictable from the adjusted input variables. As such, the model is perfectly accurate when $R^2$ is equal to 1.0, and the accuracy decreases as $R^2$ becomes smaller.

So to summarize, parameter estimation is a process where a model is fit to experimental data by minimzing the sum of the squares of the errors between the experimental and predicted responsees. Software that does this is referred to herein as a "fitting function." Typically a fitting function additionally calculates the coefficient of determination and a measure of the uncertainty in the estimated parameters (e. g. their standard errors or their 95% confidence intervals).

## Fitting Functions for a Linear Model

The parameters in a model are estimated by minimizing the sum of the squares of the errors, $\Psi$, between the experimental responses, $y_i$, and the model-predicted responses, $\hat{y}_i$, @eq-sum_of_squares_of_errors. If there is only one adjusted input variable, $x$, and the model is linear, $\hat{y}_i = mx_i + b$, the sum of the squares of the errors is given by @eq-sum_of_squares_linear, and the minimization can be performed analytically.

$$
\Psi = \sum_i \left( \left( y_i - \hat{y}_i \right)^2 \right)
$${#eq-sum_of_squares_of_errors}

$$
\Psi = \sum_i \left( \left( y_i - mx_i - b\right)^2 \right)
$${#eq-sum_of_squares_linear}

The parameters for a linear model are $m$ and $b$. The values of $m$ and $b$ that minimize $\Psi$ can be found by setting the derivative of $\Psi$ from @eq-sum_of_squares_linear with respect to each of the parameters equal to zero. Those equations can be solved algebraically to get analytical expressions for the best values of $m$ and $b$, @eq-linear_least_squares_parameters where $N$ is the number of experiments and the summations are over all experiments. 

$$
\begin{matrix}\displaystyle \frac{\partial \Psi}{\partial m} = 0 \\ \\ \displaystyle \frac{\partial \Psi}{\partial b} = 0 \end{matrix} \qquad \Rightarrow \qquad \begin{matrix} m = \displaystyle \frac{N \sum \left(x_iy_i\right) - \left(\sum x_i \right)\left(\sum y_i \right)}{N \sum\left(x_i^2\right) -\left( \sum x \right)^2}\\ \\ b = \displaystyle \frac{\sum\left(y_i\right) - m \sum\left(x_i\right)}{N} \end{matrix}
$${#eq-linear_least_squares_parameters}

The best values of the parameters in any analytic model can be found in this way, and since linear models are very common, almost any mathematics software package or spreadsheet will include a function that calculates the best slope and intercept from experimental $x-y$ data. In most cases the coefficient of determination, $R^2$, and measures of the uncertainty in the parameter values ($\lambda_m$ and $\lambda_b$ or $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$) also can be calculated. 

When implemented in spreadsheets, a model plot is typically generated. Otherwise one can be created by plotting the experimental response, $y$, *vs*. $x$ as points and, on the same axes, the model-predicted response, $\hat{y}$, *vs*. $x$ as a line. The model plot can be used to assess the accuracy of the model. When the deviations of the experimental response points from the predicted response line are small, and there are no trends in the deviations, the model is accurate.

Irrespective of the specific software package being used, a linear model fitting function must be povided with the adjusted input variables and the corresponding experimental responses for all experiments. At the minimum, a linear model fitting function will calculate and return the estimated slope, $m$, and intercept, $b$. With most software packages the linear model fitting function or associated auxillary functions will additionally calculate and return the coefficient of determination, $R^2$, and either the standard errors for the estimated paramerers, $\lambda_m$ and $\lambda_b$, or the upper and lower limits of their 95% confidence intervals, $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$.

### Estimation of Arrhenius Expression Parameters

The estimation of the parameters in the Arrhenius expression, namely the pre-exponential factor and the activation energy, is a routine task that is performed often in the analysis of kinetics data. As such, it can make sense to write a dedicated function for estimating Arrhenius expression parameters, since it can be accomplished using linear least squares.

@sec-2_rates_rate_express showed that by taking the logarithm of both sides of the Arrhenius expression, it can be converted to the linear form given in @eq-linear_arrhenius and reproduced below. @eq-x_and_y_in_linear_arrhenius shows that by defining $y = \ln{k_j}$ and $x = \frac{-1}{RT}$, the equations indeed is linear with a slope equal to $E_j$ and an intercept equal to $\ln{k_{0,j}}$

$$
\ln{k_j} = E_j \left( \frac{-1}{RT} \right) + \ln{k_{0,j}}
$$

$$
\begin{matrix} y = \ln{k_j} \\ x = \displaystyle \frac{-1}{RT} \end{matrix} \quad \Rightarrow \quad y = m x + b \quad \Rightarrow \quad \begin{matrix} m = E_j \\ b = \ln{k_{0,j}} \end{matrix}
$${#eq-x_and_y_in_linear_arrhenius}

Given a set of temperatures in absolute units, a corresponding set of rate coefficient values, and the ideal gas constant, $x$ and $y$ can be calculated for each data pair as shown in @eq-x_and_y_in_linear_arrhenius. Notice that the temperatures and the ideal gas constant must have the same temperature units. Also, the energy units for the activation energy will be the energy units of the ideal gas constant.

The resulting set of $x$ values and the corresponding set of $y$ values can be passed to a linear model fitting function of one's choosing. The linear model fitting function will return $m$, $b$, $R^2$, and either $\lambda_m$ and $\lambda_b$, or $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$.

As @eq-x_and_y_in_linear_arrhenius indicates, the activation energy is equal to the slope, so the actvation energy and its uncertainty (as its standare error or its 95% confidence interval) are given Equations [-@eq-E_linear_least_squares], [-@eq-E_lambda_linear_least_squares], [-@eq-E_uCI_linear_least_squares], and [-@eq-E_lCI_linear_least_squares].

$$
E_j = m
$$ {#eq-E_linear_least_squares}

$$
\lambda_{E_j} = \lambda_m
$$ {#eq-E_lambda_linear_least_squares}

$$
CI_{u,E_j} = CI_{u,m}
$$ {#eq-E_uCI_linear_least_squares}

$$
CI_{l,E_j} = CI_{l,m}
$$ {#eq-E_lCI_linear_least_squares}

@eq-x_and_y_in_linear_arrhenius shows that the intercept, $b$, is the logarithm of the pre-exponential factor, so equations [-@eq-k0_linear_least_squares], [-@eq-k0_lambda_linear_least_squares], and [-@eq-k0_CI_linear_least_squares] must be used to calculate $k_{0,j}$ and its uncertainty.

$$
k_{0,j} = \exp {\left(b\right)}
$${#eq-k0_linear_least_squares}

$$
\lambda_{k_{0,j}} = k_{0,j} \exp {\left(\lambda_b\right)}
$${#eq-k0_lambda_linear_least_squares}

$$
\begin{aligned}
CI_{u,k_{0,j}} &= \exp{\left( CI_{u,b} \right)} \\
CI_{l,k_{0,j}} &= \exp{\left( CI_{l,b} \right)}
\end{aligned}
$${#eq-k0_CI_linear_least_squares}

A dedicated function for estimating Arrhenius expression parameters then can be structured as follows.

1. It should receive a vector of $k$ values, a corresponding vector of $T$ values (in absolute units), and the value of the ideal gas constant, $R$, in the same temperature units and the energy units desired for the activation energy as arguments
2. It should calculate vectors of $x$ and $y$ using equation @eq-x_and_y_in_linear_arrhenius.
3. It should calculate $m$, $b$, $R^2$, and either $\lambda_m$ and $\lambda_b$ or  $CI_{u,m}$, $CI_{u,b}$, $CI_{l,m}$, and $CI_{l,l}$ by calling a linear model fitting function providing the $x$ and $y$ vectors.
4. It should calculate $k_{0,j}$, $E_j$, and either $\lambda_{k_{0,j}}$ and $\lambda_{E_j}$ or $CI_{u,k_{0,j}}$, $CI_{l,k_{0,j}}$, $CI_{u,E_j}$, and $CI_{l,E_j}$ using Equations [-@eq-E_linear_least_squares], [-@eq-E_lambda_linear_least_squares], [-@eq-E_uCI_linear_least_squares],  [-@eq-E_lCI_linear_least_squares], [-@eq-k0_linear_least_squares], [-@eq-k0_lambda_linear_least_squares], and [-@eq-k0_CI_linear_least_squares].
5. It should return $k_{0,j}$, $E_j$, $R^2$, and either $\lambda_{k_{0,j}}$ and $\lambda_{E_j}$ or $CI_{u,k_{0,j}}$, $CI_{l,k_{0,j}}$, $CI_{u,E_j}$, and $CI_{l,E_j}$.

## Fitting Functions for a Numerical Model

In *Reaction Engineering Basics*, reactor models are solved numerically. Therefore a fitting function that can use a numerical model must be used for parameter estimation. Many software packages include this kind of fitting function. An overview of the use of fitting functions for numerical models is presented in @sec-6_kin_data_gen. The flow of information is diagram from that discussion is reproduced here as @fig-apndx_L_param_est_info_flow. This section offers an abbreviated description of how these functions work.

![Information flow for fitting a reactor model to an experimental data set using a computer fitting function.](Graphics/parameter_estimation_info_flow.png){#fig-apndx_L_param_est_info_flow width="70%"}

The parameters are still estimated by minimizing the sum of the squares of the errors, @eq-sum_of_squares_of_errors, but the minimization is performed numerically and not analytically. Regardless of the specific software being used, four things must be provided to the fitting function, as indicated in @fig-apndx_L_param_est_info_flow. Those inputs to the fitting function are

1. An initial guess for the values of the model parameters.
2. The values of the adjusted input variables for all of the experiments being analyzed.
3. The corresponding values of the experimental responses.
4. A responses function, written by the user, that uses the model to calculate and return the corresponding model-preducted responses.
    a. The responses function receives a guess for the model parameters and the values of adjusted input variables for all of the experiments and it returns the model-predicted responses for all of the experiments.

A fitting function works in much the same manner as an ATE solver (see @sec-apndx_solve_ates) except that instead of finding values of unknowns that cause a set of residuals to equal zero it finds values of model parameters that minimize the sum of the squares of the errors. In essence, the fitting function finds the best parameter values by trial and error.

* It creates variables to hold the best parameter estimates and the corresponding best sum of the squares of the errors.
* It calls the responses function to get the model predicted responses and then calculates $\Psi$ using @eq-sum_of_squares_of_errors.
* It saves the initial guess and the corresponding $\Psi$ as the best values.
* It repeatedly 
    * generates an improved guess 
    * calculates $\Psi$ as above, and 
    * if $\Psi$ is less than the best $\Psi$ it saves the improved guess and corresponding $\Psi$ as the best values.
* It stops generating new guesses when it is unable to generate an improved guess that results in a smaller $\Psi$.
* It calculates $R^2$, and either the standard errors or the 95% confidence interval for each parameter.
* It returns the estimated parameter values, $R^2$, and either the standard errors or the 95% confidence interval for each parameter.

### Convergence

Parameter estimation as described above is an iterative process. Ideally, as improved guesses are generated, the corresponding sum of the squares of the errors gets smaller and smaller. This is called **convergence**. The fitting function uses a set of convergence criteria to determine whether (a) it has converged to a minimum value of the sum of the squares of the errors, (b) it has not converged to a minimum value of the sum of the squares of the errors but it is making progress and should continue iterating or (c) it has not converged to a minimum value and it is not making progress, so it should stop iterating. Typical convergence criteria are listed below; the fitting function usually provides a default for the "specified amounts" in this list, but the defaults can be overridden at the time the fitting function is called. 

* the sum of the squares of the errors is smaller than a specified amount (converged).
* the sum of the squares of the errors is smaller than it was for the previous iteration (making progress)
* a (large) specified number of iterations has occurred (not making progress).
* the sum of the squares of the errors have been calculated a specified number of times (not making progress).
* the sum of the squares of the errors is increasing instead of decreasing (not making progress).
* the sum of the squares of the errors is changing by less than a specified amount between iterations (not making progress).
* the guess is changing by less than a specified amount between iterations (not making progress).

### Caveats

A few issues should be kept in mind when performing numerical parameter estimation. The first is that **the solver may converge to a local minimum** of the sum of the squares of the errors and not the global minimum. This could result in a model that does not appear to be accurate, but that could be accurate if the global minimum was located. One way to try to detect this situation is to repeat the parameter estimation using a very different initial guess. If the solver converges to a different set of estimated parameters, that indicates that one (or possibly both) of the sets of estimated parameters corresponds to a local minimum. If a wide range of initial guesses always leads to the same parameter estimates, that *may* suggest the a global minimum has been found, and it is the model, itself, that is not accurate.

The second issue arises **when initially guessing the value of a model parameter that might fall anywhere in a very wide range of values**. As an example, in *Reaction Engineering Basics* problems, the pre-exponential factor for a rate coefficient could have a value anywhere between 10^-20^ and 10^20^. In this situation, if the initial guess is not sufficiently close to the actual value of the parameter, the solver may quit because it is not making progress. One way to reduce the likelihood of this happening is to use the base-10 logarithm of the parameter instead of the parameter itself. That is, if $k_0$ is the actual parameter of interest in the model, re-write the model replacing $k_0$ with $10^\beta$. Then perform parameter estimation to find the best value of $\beta$. When the possible range of $k_0$ is between 10^-20^ and 10^20^, the corresponding range of $\beta$ is between -20 and 20. Once the best value $\beta$ has been found, the best value of $k_0$ is simply calculated as $k_0 = 10^\beta$. 

The third issue arises **when the experimental responses span several orders of magnitude**. In this situation, the errors associated with larger values of the response may be greater than errors associated with smaller values of the response. If that is so, the minimization process will focus on the responses with larger values because they have a greater effect upon the sum of the squares of the errors. One option for addressing this situation is to minimize the sum of the squares of the *relative* errors instead of the sum of the squares of the *absolute* errors.

## Assessing Model Accuracy

Typically, if successful, the solver will return (a) the estimated values of the model parameters, (b) some measure of the uncertainty in the estimated values (e. g. a 95% confidence interval) and (c) the coefficient of determination, $R^2$. However, finding the best values for the parameters does not ensure that the model is accurate (see @fig-line_fit_to_parabola).

When the model being fit to the data is linear and has only one adjusted input variable a model plot can be created as described above. Otherwise, the estimated values of the model parameters can be used to compute the model-predicted response for each experiment. A "parity plot" can then be created wherein the model-predicted response is plotted versus the experimental response. If the model was perfect, every point in the parity plot would fall on a diagonal line passing through the origin. The farther the points are from the diagonal, the lower the accuracy of the model.

The errors, or residuals, for each experiment can also be calculated. A set of "residuals plots" can then be created wherein the residuals are plotted versus each of the experimental adjusted input variables. If the model was perfect, every residual would equal zero and every point in every residuals plot would fall on the horizontal axis. The magnitude of the deviations from the horizontal axis is not important (the parity plot is used for this purpose). The critical issue whether the points scatter randomly above and below the horizontal axis as the value of the adjusted input variable increases. When the model is good, there should not be any trends in the deviations as the value of the adjusted input variable increases.

Deciding whether the model is sufficiently accurate is a judgement call. The following criteria are satisfied by an accurate model.

* The coefficient of determination, $R^2$, is nearly equal to 1.
* The uncertainty in most, if not all, of model parameters is small compared to the parameter's value.
    * When using standard errors of the parameters, they are small compared to the parameter value.
    * When using 95% confidence intervals, the upper and lower limits of the interval are close to the parameter value.
* The points in the parity plot are all close to a diagonal line passing through the origin.
* In each residuals plot, the points scatter randomly about the horizontal axis, and no systematic deviations are apparent.

As noted in @sec-6_kin_data_gen, the uncertainty in *most*, but not *all*, of the parameters might be small. This could indicate one of three possibilities. First, the factor levels used in the experiments may not allow accurate resolution of the parameters with high uncertainty. Second, the parameters with higher uncertainty may be mathematically coupled to other parameters (e. g. the model-predicted response may only depend on the product of two parameters so that the individual parameters can have any values as long as their product has the optimum value). Alternatively, the parameters with high uncertainty may not be needed, and there may be a simpler model with fewer parameters that is equally accurate.

## Mathematical Formulation of Parameter Estimation

In *Reaction Engineering Basics* the unknown parameters will always be constants that appear in the rate expression. They will always be calculated by calling a fitting function, and a responses function will always need to be written and provided to it. Generally the responses function will loop through the experiments. For each experiment it will solve the reactor design equations for the experimental reactor and use the results to calculate the model-predicted response.

* identify all known constants and their values
* formulate a reactor model for the experimental reactor assuming the rate expression parameters and the adjusted input variables will be available.
* write any ancillary equations that are needed to solve the reactor model
* write any ancillary equations that are needed to calculate the model-predicted response given the solution of the reactor model equations

## Numerical Implementation of Parameter Estimation

* create a matrix of adjusted input variables with one column for each variable and one row for each experiment
* create a column vector of experimental responses with one row for each experiment and with the rows in the same order as in the adjusted input variable matrix
* create a vector containing a guess for each unknown parameter

## Examples

### Arrhenius Expression Parameters ... {#sec-example_L_7_1}

### to be added {#sec-example_L_7_2}

## Symbols Used in @sec-apndx_parameter_est
